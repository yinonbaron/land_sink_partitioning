{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate estimates for change in total carbon stocks, living biomass and non-living carbon stocks from observations and DGVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The following notebook take all of the data produced in previous steps and generate from it two main types of estimates for two different data types. The two kinds of estimates we produce are:\n",
    "\n",
    "- decadal estimates of change in total carbon stocks, living biomass and non-living carbon stocks\n",
    "\n",
    "- annual estimates of cummulative change in total carbon stocks, living biomass and non-living carbon\n",
    "\n",
    "The two data types we analyze are observation-based estimates and DGVM-based estimates. \n",
    "\n",
    "For each type of estimate and data type, we generate both mean estimates as well as uncertainty estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Observation-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Total carbon stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global\n",
    "\n",
    "# load global data\n",
    "GCB_data = pd.read_csv('../results/05_estimate_land_sink/global.csv',index_col=[0])\n",
    "GCB_data = GCB_data.loc[start_year:2019]\n",
    "GCB_data.loc[start_year,'land_sink'] = 0 \n",
    "\n",
    "# anthropogenic perturbation of the land to ocean flux from Regnier et al. 2022\n",
    "F_lat = 0.05e15\n",
    "F_lat_std = 0.025e15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional\n",
    "\n",
    "# load regionall NEE and FF fluxes\n",
    "NEE = pd.read_csv('../results/05_estimate_land_sink/NEE_regional.csv',index_col=[0,1])\n",
    "FF = pd.read_csv('../results/05_estimate_land_sink/FF_regional.csv',index_col=[0,1])\n",
    "NEE.columns.name = 'year'\n",
    "FF.columns.name = 'year'\n",
    "NEE.columns = NEE.columns.astype(int)\n",
    "FF.columns = FF.columns.astype(int)\n",
    "\n",
    "# take only years between 1993 and 2019\n",
    "NEE = NEE.loc[:,start_year+1:2019]\n",
    "FF = FF.loc[:,start_year+1:2019]\n",
    "\n",
    "# load lateral trade fluxes from Ciais et al. 2021\n",
    "trade_data = pd.read_excel('../data/carbon_cycle/lateral_fluxes/ciais_et_al_2021/data.xlsx')\n",
    "\n",
    "# take only the trade fluxes and convert from TgC to PgC. For the uncertainty convert to gC\n",
    "trade_std = trade_data[(trade_data.Region!='Globe') & (trade_data.Parameter=='Ftrade')].set_index('Region')['Uncertainty estimation']/1e3*1e15\n",
    "trade_data = trade_data[(trade_data.Region!='Globe') & (trade_data.Parameter=='Ftrade')].set_index('Region')['Value']/1e3\n",
    "\n",
    "\n",
    "# load lateral river fluxes\n",
    "river_data = pd.read_csv('../results/05_estimate_land_sink/river_fluxes.csv',index_col=0)['river_flux']/1e15\n",
    "\n",
    "F_lateral = (river_data +trade_data.reindex(river_data.index,fill_value=0))*1e15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Living biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "biomass_data_obs = pd.read_csv('../results/04_temporal_harmonization/harmonized_biomass_data.csv',index_col=[0,1,2,3])\n",
    "biomass_data_obs.columns = biomass_data_obs.columns.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. DGVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load regional nbp and cVeg data\n",
    "trendy_nbp_regional = pd.read_csv('../results/03_aggregate_regions/regional_DGVM_nbp.csv',index_col=[0,1])\n",
    "trendy_cVeg_regional = pd.read_csv('../results/03_aggregate_regions/regional_DGVM_cVeg.csv',index_col=[0,1])\n",
    "trendy_nbp_regional.columns = trendy_nbp_regional.columns.astype(int)\n",
    "trendy_cVeg_regional.columns = trendy_cVeg_regional.columns.astype(int)\n",
    "\n",
    "# calculate the time difference and select the time period between 1993-2019\n",
    "trendy_cVeg_regional = trendy_cVeg_regional.diff(axis=1)\n",
    "trendy_cVeg_regional = trendy_cVeg_regional.loc[:,start_year+1:2019]\n",
    "trendy_nbp_regional = trendy_nbp_regional.loc[:,start_year+1:2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load global estimates from the models\n",
    "trendy_global_nbp = pyreadr.read_r('../data/DGVMs/nbp_S3.RData')['nbp_S3'].T*1e15\n",
    "trendy_global_cVeg = pyreadr.read_r('../data/DGVMs/cVeg_S3.RData')['cVeg_S3'].T*1e15\n",
    "\n",
    "# convert units from kgC per second to PgC per year\n",
    "trendy_global_nbp = trendy_global_nbp*(3600*24*365*1e3/1e15)\n",
    "\n",
    "# set model names and years from the regional data\n",
    "trendy_global_nbp.index = trendy_nbp_regional[start_year+1].unstack().columns\n",
    "trendy_global_nbp.columns = np.arange(1901,2021)\n",
    "trendy_global_cVeg.index = trendy_nbp_regional[start_year+1].unstack().columns\n",
    "\n",
    "\n",
    "# drop out missing data\n",
    "trendy_global_cVeg = trendy_global_cVeg.dropna()\n",
    "\n",
    "# take time difference for cVeg data\n",
    "trendy_global_cVeg = trendy_global_cVeg.diff(axis=1)\n",
    "trendy_global_cVeg = trendy_global_cVeg.dropna(axis=1)\n",
    "\n",
    "# set the columns of the cVeg data\n",
    "trendy_global_cVeg.columns = np.arange(1902,2021)\n",
    "\n",
    "# take only the times in the study period\n",
    "trendy_global_nbp = trendy_global_nbp.loc[:,start_year+1:2019]\n",
    "trendy_global_cVeg = trendy_global_cVeg.loc[:,start_year+1:2019]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define functions for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define time bins\n",
    "time_bins = pd.DataFrame(pd.Series([np.arange(start_year+1,2001),np.arange(2001,2011),np.arange(2011,2020)]),columns=['year'])\n",
    "\n",
    "# create time bin names\n",
    "time_bin_names = time_bins.apply(lambda x: '-'.join([str(x.iloc[0].min()),str(x.iloc[0].max())]),axis=1)\n",
    "\n",
    "# set the index to be the names\n",
    "time_bins.index = time_bin_names\n",
    "\n",
    "# calculate the frequency of each time bin\n",
    "year_bin_freq = (time_bins.apply(lambda x: len(x['year']),axis=1)/time_bins.apply(lambda x: len(x['year']),axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_regional_stat(df:pd.DataFrame, year_bins:list, stat:str,grouping_vars=['inversion','region']) -> pd.DataFrame:\n",
    "    '''\n",
    "    Function to calculate regional statistics .\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): data\n",
    "    year_bins (list): list of year bins\n",
    "    stat (str): statistic to calculate (mean or std)\n",
    "    grouping_vars (list): list of variables to group by\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: regional statistics\n",
    "    '''\n",
    "\n",
    "    # extract the names of the time periods\n",
    "    period_names = ['-'.join([str(j+1),str(year_bins[i+1])]) for i,j in enumerate(year_bins[:-1])]\n",
    "\n",
    "    # check if the stat is either mean or std\n",
    "    assert stat in ['mean','std'], 'stat must be either mean or std'\n",
    "    \n",
    "    # copy the input data (for some reason the analysis overwrites the input data)\n",
    "    df_time_mean = df.copy()\n",
    "\n",
    "    # set the columns to the respective time bin\n",
    "    df_time_mean.columns = pd.cut(df_time_mean.columns,bins=year_bins)\n",
    "\n",
    "    # set the columns name to period\n",
    "    df_time_mean.columns.name = 'period'\n",
    "\n",
    "    # calculate the regional mean and number of years for each inversion\n",
    "    df_time_mean = df_time_mean.stack().groupby(grouping_vars+['period'],observed=False).agg(['mean','count'])\n",
    "    \n",
    "    # remove methods with only one measurement in a given year bin\n",
    "    df_time_mean = df_time_mean.loc[df_time_mean['count']>1]\n",
    "\n",
    "    # if the grouping_vars are larger than 1, take the last variable as the grouping variable\n",
    "    gb2 = grouping_vars[-1:] if len(grouping_vars) >1 else grouping_vars\n",
    "    \n",
    "    # calculate the regional mean or std across the different inversion for the same year bin\n",
    "    df_regional_stats = df_time_mean.groupby(gb2 + ['period'],observed=False)['mean'].agg(stat).unstack()\n",
    "\n",
    "    # set the column names to be the nice period names\n",
    "    df_regional_stats.columns = period_names\n",
    "\n",
    "    return df_regional_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Total carbon stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_land_std(land_df,start:int, end:int, ocean_random:float, ocean_sys:float,F_lat_std=F_lat_std) -> float:\n",
    "    '''\n",
    "    Function to calculate the uncertainty of the net land sink for a given time period.\n",
    "\n",
    "    Parameters:\n",
    "    land_df (pd.DataFrame): land sink data\n",
    "    start (int): start year of the time period\n",
    "    end (int): end year of the time period\n",
    "    ocean_random (float): random uncertainty of the ocean sink\n",
    "    ocean_sys (float): systematic uncertainty of the ocean sink\n",
    "    F_lat_std (float): uncertainty of the anthropogenic perturbation of the lateral flux from land to ocean\n",
    "\n",
    "    Returns:\n",
    "    float: uncertainty of the net land sink\n",
    "    '''\n",
    "\n",
    "    # define component stds\n",
    "    FF_std = land_df.loc[start:end,'FF_std'].mean()\n",
    "    AGR_std = land_df.loc[start:end,'AGR_std'].mean()\n",
    "    cement_std = land_df.loc[start:end,'cement_std'].mean()\n",
    "\n",
    "    ocean_random = ocean_random/(end-start) # calculate the standard deviation of the random ocean uncertainty for the mean flux across the period\n",
    "    conponent_std = np.array([FF_std,AGR_std,cement_std,ocean_random,ocean_sys,F_lat_std])\n",
    "\n",
    "    # propogate uncertainties\n",
    "    land_sink_std = np.sqrt((conponent_std**2).sum())\n",
    "\n",
    "    return land_sink_std    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Living biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist(df:pd.DataFrame,time_bin:np.array,bin_vars:list,n_samples=1000,stat='mean') -> list:\n",
    "    \"\"\"\n",
    "    Create random samples from the data for each time bin. For each sample, we choose randomly one method from each source and then one source.\n",
    "    If the original sample has missing values, we fill them with the values from the second sample. \n",
    "\n",
    "    Parameters:\n",
    "    df: pd.DataFrame\n",
    "        The data frame with the data\n",
    "    time_bin: np.array\n",
    "        The time bin to sample from\n",
    "    bin_vars: list\n",
    "        The variables to use as columns\n",
    "    n_samples: int\n",
    "        The number of samples to create\n",
    "    stat: str\n",
    "        The statistic to calculate in each sample (mean or cumsum)\n",
    "\n",
    "    Returns:\n",
    "    list\n",
    "        A list with the samples\n",
    "    \"\"\"\n",
    "    \n",
    "    # check if the stat is either mean or cumsum\n",
    "    assert stat in ['mean','cumsum'], 'stat must be either mean or cumsum'\n",
    "    \n",
    "    if stat == 'mean':\n",
    "        # select the data that for the years in time_bin, drop missing values and calculate make the bin_vars as columns\n",
    "        df_tb = df.loc[:,df.columns.isin(time_bin)].mean(axis=1).dropna().unstack(bin_vars)\n",
    "    else:\n",
    "        df_tb = df.loc[:,df.columns.isin(time_bin)].cumsum(axis=1)\n",
    "        df_tb = df_tb[df_tb.sum(axis=1)!=0].unstack(bin_vars)\n",
    "\n",
    "    # define the result list\n",
    "    res = []\n",
    "\n",
    "    # for each sample\n",
    "    for i in range(n_samples):\n",
    "        \n",
    "        # sample one method from each source, then sample one source and take the values\n",
    "        sample = df_tb.groupby('source').sample(1).sample(1).T.iloc[:,0]\n",
    "\n",
    "        # while the sample doesn't cover all regions and lancovers (has nan values)\n",
    "        while sample.isna().any():\n",
    "            # sample again\n",
    "            sample2 = df_tb.groupby('source').sample(1).sample(1).T.iloc[:,0]\n",
    "            \n",
    "            # merge the two samples\n",
    "            merged_samples = pd.concat([sample,sample2],axis=1,keys=['first_sample','second_sample'])\n",
    "\n",
    "            # fill the missing values in the original sample with the values from the second sample\n",
    "            sample = merged_samples['first_sample'].fillna(merged_samples['second_sample'])\n",
    "                \n",
    "        # add the sample to the result list\n",
    "        res.append(sample)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def sort_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sort the columns of a data frame based on the sum of the values in the columns\n",
    "\n",
    "    Parameters:\n",
    "    df: pd.DataFrame\n",
    "        The data frame to sort\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        The sorted data frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # sort the columns based on the sum of the values\n",
    "    sorted_df = df.loc[:,df.sum().sort_values(ascending=False).index]\n",
    "    \n",
    "    # rename columns\n",
    "    sorted_df.columns = range(df.shape[1])\n",
    "    return sorted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_std_df(trajectories:pd.DataFrame,year_bin_freq:pd.Series) -> pd.DataFrame:\n",
    "    '''\n",
    "    Function to calculate the regional and global uncertainty for different time periods from the given random samples.\n",
    "\n",
    "    Parameters:\n",
    "    trajectories (pd.DataFrame): the random samples\n",
    "    year_bin_freq (pd.Series): the frequency of each time bin\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: the standard deviation of the data for the different time periods\n",
    "    '''\n",
    "\n",
    "    # sum over the landcovers to get regional trajectories\n",
    "    trajectories_regional = trajectories.groupby(['time_bin','region']).sum()\n",
    "\n",
    "    # add random noise to the data with a certain coefficient of variation\n",
    "    cv = 0.3 # typical number from the literature\n",
    "    trajectories_regional = trajectories_regional + np.random.normal(0,cv*trajectories_regional.abs())\n",
    "\n",
    "    # calculate the standard deviation between samples as a measure of unertainty\n",
    "    std_df = trajectories_regional.std(axis=1).unstack()\n",
    "\n",
    "    # for the entire period, calculate the overall uncertainty assuming no correlation between time periods\n",
    "    std_df.loc[f'{start_year+1}-2019'] = np.sqrt(std_df.T**2 @ year_bin_freq**2)\n",
    "\n",
    "    # Global\n",
    "    # sum over the landcovers to get regional trajectories\n",
    "    trajectories_global = trajectories.groupby('time_bin').sum()\n",
    "\n",
    "    # add random noise to the data with a certain coefficient of variation\n",
    "    cv = 0.3 # typical number from the literature\n",
    "    trajectories_global = trajectories_global + np.random.normal(0,cv*trajectories_global.abs())\n",
    "\n",
    "    # calculate the standard deviation between samples as a measure of unertainty\n",
    "    global_std = trajectories_global.std(axis=1)\n",
    "\n",
    "    # for the entire period, calculate the average rate across overall uncertainty assuming no correlation between time periods\n",
    "    global_std.loc[f'{start_year+1}-2019'] = (trajectories_global.T @ year_bin_freq).std()\n",
    "\n",
    "    # combine the regional and global estimates\n",
    "    std_df['Global'] = global_std\n",
    "\n",
    "    return std_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. DGVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_global(df:pd.DataFrame, time_bins_edges:list) -> pd.Series:\n",
    "    '''\n",
    "    Function to calculate the global mean and standard deviation for different time periods of DGVM data.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): the data\n",
    "    time_bins_edges (list): the edges of the time bins\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: the global mean and standard deviation for the different time periods\n",
    "    '''\n",
    "    \n",
    "    # extract the names of the time periods\n",
    "    period_names = ['-'.join([str(j+1),str(time_bins_edges[i+1])]) for i,j in enumerate(time_bins_edges[:-1])]\n",
    "\n",
    "    # define the result variable\n",
    "    global_df = pd.DataFrame(np.nan,index= period_names,columns=['mean','std'])\n",
    "\n",
    "    # for each time period calculate the mean and std across the models\n",
    "    for i,start,end in zip(range(len(time_bins_edges)-1),time_bins_edges[:-1],time_bins_edges[1:]):\n",
    "        \n",
    "        global_df.iloc[i] = df.loc[:,start+1:end].mean(axis=1).agg(['mean','std'])\n",
    "\n",
    "    # calculate the mean and std across the models for the entire period\n",
    "    global_df.loc['1993-2019'] = df.mean(axis=1).agg(['mean','std'])\n",
    "    return global_df\n",
    "\n",
    "def analyze_DGVM(df:pd.DataFrame, global_df:pd.DataFrame, time_bins_edges:list, stat:str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Function to analyze the DGVM data for different time periods.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): the data\n",
    "    global_df (pd.DataFrame): the global data\n",
    "    time_bins_edges (list): the edges of the time bins\n",
    "    stat (str): the statistic to calculate (mean or std)\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: the regional and global statistics for the different time periods\n",
    "    '''\n",
    "\n",
    "    assert stat in ['mean','std'], 'stat must be either mean or std'\n",
    "\n",
    "    # calculate regional stat\n",
    "    res = calc_regional_stat(df,time_bins_edges,stat,['model','region'])\n",
    "    \n",
    "    # calculate the statistic for the entire period\n",
    "    res[f'{start_year+1}-2019'] = df.mean(axis=1).groupby('region').agg(['mean','std'])[stat]\n",
    "    \n",
    "    # calculate the statistic for the globe\n",
    "    res.loc['Global',:] =  calc_global(global_df,time_bins_edges)[stat]\n",
    "\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Observation-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1. Total carbon stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1.1. Global estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The net land flux is calculated according to the following formula:\n",
    "\n",
    "$$ F_{land} = E_{FOS} - S_{O} - AGR - S_{cement} $$\n",
    "\n",
    "Where $F_{land}$ is the net land sink, $E_{FOS}$ is the emissions from fossil fuels, $S_{O}$ is the sink from the ocean, $AGR$ is the atmospheric growth rate, and $S_{cement}$ is the sink from cement carbonation.\n",
    "\n",
    "To calculate from the net land sink the change in total carbon stocks on land, we remove an estimate for the anthropogenic perturbation of the lateral fluxes between land and the ocean of ≈0.05 GtC <sup>-1</sup> based on [Regnier et al. (2022)](https://www.nature.com/articles/s41586-021-04339-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the net land sink\n",
    "GCB_data['land_sink'] = GCB_data['FF'] - GCB_data['AGR'] - GCB_data['ocean'] - GCB_data['cement']\n",
    "\n",
    "# calculate the mean net land sink for the different time bins\n",
    "global_land = time_bins.apply(lambda x: GCB_data.loc[x['year'],'land_sink'].mean(),axis=1)\n",
    "global_land.index = time_bin_names\n",
    "global_land.loc[f'{start_year+1}-2019'] = GCB_data.loc[start_year+1:2019,'land_sink'].mean()\n",
    "\n",
    "# remove 0.05 anthropogenic perturbation of the land to ocean flux (F'_EC) from Regnier et al. 2022\n",
    "global_land = global_land - F_lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the uncertainty of $F_{land}$ at each year we need to combine the uncertainties of each component based on estimates from the Global Carbon Budget report ([Friedlingstein et al. 2023](https://essd.copernicus.org/articles/15/5301/2023/)), assuming that the uncertainties reported represent systematic errors.\n",
    "\n",
    "For the ocean sink, the overall uncertainty is ≈0.4 GtC yr<sup>-1</sup>, but it is composed of random and systematic errors. The random errors which will be reduced when making averages and to a systematic part which will not be reduced.\n",
    "\n",
    "The ocean sink is calculated using two methods - Global Ocean Biogeochemical Models (GOBMs) and data products. \n",
    "The random error for both methods is 0.3 GtC yr<sup>-1</sup> each. \n",
    "The systematic uncertainty of GOBMs is 0.4 GtC yr<sup>-1</sup>. \n",
    "By propagating uncertainties from observations (0.2 GtC yr<sup>-1</sup>), \n",
    "gas-transfer velocity (0.2 GtC yr<sup>-1</sup>), \n",
    "wind product (0.1 GtC yr<sup>-1</sup>), \n",
    "river flux adjustment (0.3 GtC yr<sup>-1</sup>), \n",
    "and $fCO_2$ mapping (0.2 GtC yr<sup>-1</sup>), \n",
    "we calculate the systematic uncertainty of the data-products to be $\\sqrt{0.2^2 + 0.2^2 + 0.1^2 + 0.3^2 + 0.2^2}$ ≈ 0.5 GtC yr<sup>-1</sup>. \n",
    "Using these estimates, we can calculate the random and systematic components of the global ocean sink estimate:\n",
    "\n",
    "- random error: $\\sqrt{0.5^2 * 0.3^3+0.5^2 * 0.3^2}$ = 0.21\n",
    "- systematic error: $\\sqrt{0.5^2 * 0.4^2 + 0.5^2 * 0.47^2}$ = 0.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate random and systematic ocean uncertainty\n",
    "GOBM_random = 0.3e15\n",
    "GOBM_sys = 0.4e15\n",
    "data_prod_random = 0.31e15\n",
    "data_prod_sys = 0.5e15\n",
    "ocean_random = np.sqrt(GOBM_random**2*0.5**2 + data_prod_random**2*0.5**2)\n",
    "ocean_sys = np.sqrt(GOBM_sys**2*0.5**2 + data_prod_sys**2*0.5**2)\n",
    "\n",
    "# calculate the uncertainty of the net land sink for different time periods\n",
    "global_land_std = time_bins.apply(lambda x: calc_land_std(GCB_data,x['year'].min(),x['year'].max(),ocean_random,ocean_sys),axis=1)\n",
    "global_land_std.index = time_bin_names\n",
    "global_land_std.loc[f'{start_year+1}-2019'] = calc_land_std(GCB_data,start_year+1,2019,ocean_random,ocean_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate annual estimates\n",
    "\n",
    "# calculate mean\n",
    "delta_C_obs_annual_cum = (GCB_data.loc[start_year+1:2019,'land_sink']-F_lat).cumsum() \n",
    "delta_C_obs_annual_cum[start_year] = 0\n",
    "delta_C_obs_annual_cum.sort_index(inplace=True)\n",
    "\n",
    "# calculate uncertainty\n",
    "delta_C_obs_annual_std_cum = np.zeros(GCB_data.loc[start_year:].shape[0])\n",
    "for i,e in enumerate(GCB_data.loc[start_year+1:].index):\n",
    "    delta_C_obs_annual_std_cum[i+1] = calc_land_std(GCB_data,start_year,e,ocean_random,ocean_sys)*(e-start_year)\n",
    "\n",
    "delta_C_obs_annual_std_cum = pd.Series(delta_C_obs_annual_std_cum,index=GCB_data.loc[start_year:].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1.2. Regional estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the mean regional total carbon stock change, we rely on the mean of the 14 different inversions we use. We bin the data in 4 time bins to keep the coverage of different inversions consistent across each time bin. \n",
    "\n",
    "We remove from the vertical exchange between land and the atmosphere the lateral fluxes between each region based on estimates of lateral fluxes into the ocean and lateral trade across RECCAP regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_bins = np.array([start_year,2000,2010,2014,2019])\n",
    "\n",
    "# calculate the mean regional NEE\n",
    "NEE_regional_mean = calc_regional_stat(NEE,year_bins,'mean')\n",
    "\n",
    "# calculate the time length of each year bin\n",
    "bin_length = year_bins[1:]-year_bins[:-1]\n",
    "\n",
    "# calculate the regional mean change in total carbon stocks by removing from NEE lateral fluxes\n",
    "nbp_regional_mean = -NEE_regional_mean.add(F_lateral,axis=0).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For calculating the uncertainty of the regional changes in the total carbon stocks on land, we calculate three different uncertainty components and propagate their respective uncertainty to the final estimate:\n",
    "\n",
    "1. The variability between the estimates of different inversions on the average rate of exchange of CO<sub>2</sub> between the atmosphere and the land surface. We calculate the standard deviation of the mean of the 14 inversions for each region and time bin.\n",
    "\n",
    "2. The uncertainty associated with lateral fluxes - we rely on the uncertainty associated with land to ocean fluxes from the Glboal Carbon Budget [Friedlingstein et al. (2023)](https://essd.copernicus.org/articles/15/5301/2023/) of ≈0.3 GtC yr<sup>-1</sup> and the uncertainty associated with lateral trade from [Ciais et al. 2021](https://academic.oup.com/nsr/article/8/2/nwaa145/5868251).\n",
    "\n",
    "3. Uncertainty of regional fossil fuel emissions, contribute to the uncertainty of each single inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. calculate the mean regional NEE\n",
    "NEE_regional_std = calc_regional_stat(NEE,year_bins,'std')\n",
    "\n",
    "# take land to ocean uncertainty from the Global Carbon Budget\n",
    "river_std = 0.3e15\n",
    "\n",
    "# scale the uncertainty of the lateral fluxes to the total river flux\n",
    "river_std = river_data/river_data.sum()*river_std\n",
    "\n",
    "# calculate the uncertainty of the lateral fluxes\n",
    "lat_std = np.sqrt(river_std**2 + trade_std**2)\n",
    "\n",
    "# 3. calculate the uncertainty of the fossil fuel flux assuming 5% uncertainty\n",
    "FF_std = calc_regional_stat(FF,year_bins,'mean')*0.05\n",
    "\n",
    "# calculate the overall uncertainty of the carbon stock change\n",
    "nbp_regional_std = np.sqrt((NEE_regional_std**2 + FF_std**2).add(lat_std**2,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1.3. Combine global and regional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "# take the regional estimates for 1993-2000 and 2001-2010\n",
    "delta_C_obs = nbp_regional_mean.iloc[:,:2].T\n",
    "delta_C_obs.index = time_bin_names[:2]\n",
    "\n",
    "# for the 2011-2019 period, calculate the mean regional change by the weighted average of the rates for 2011-2014 and 2015-2019\n",
    "delta_C_obs.loc['2011-2019'] = nbp_regional_mean.iloc[:,2:] @ (bin_length[2:]/sum(bin_length[2:]))\n",
    "\n",
    "# for the 1993-2019 period, calculate the mean regional change by the weighted average of all periods\n",
    "delta_C_obs.loc[f'{start_year+1}-2019'] = nbp_regional_mean @ (bin_length/sum(bin_length))\n",
    "\n",
    "# set the global estimates from the result of the global analysis\n",
    "delta_C_obs['Global'] = global_land\n",
    "\n",
    "#std\n",
    "# take the regional estimates for 1993-2000 and 2001-2010\n",
    "delta_C_obs_std = nbp_regional_std.iloc[:,:2].T\n",
    "delta_C_obs_std.index = time_bin_names[:2]\n",
    "\n",
    "# for the 2011-2019 period, propagate uncertainties assuming periods are uncorrelated by using the weighted average of the variances for 2011-2014 and 2015-2019\n",
    "delta_C_obs_std.loc['2011-2019'] = np.sqrt(nbp_regional_std.iloc[:,2:]**2 @ (bin_length[2:]/sum(bin_length[2:]))**2)\n",
    "\n",
    "# for the 1993-2019 period, propagate uncertainties assuming periods are uncorrelated by using the weighted average of the variances for all periods\n",
    "delta_C_obs_std.loc[f'{start_year+1}-2019'] = np.sqrt(nbp_regional_std**2 @ (bin_length/sum(bin_length))**2)\n",
    "\n",
    "# set the global uncertainty estimates from the result of the global analysis\n",
    "delta_C_obs_std['Global'] = global_land_std\n",
    "\n",
    "# transpose data to make it consistent with other datasets\n",
    "delta_C_obs = delta_C_obs.T\n",
    "delta_C_obs_std = delta_C_obs_std.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2. Living biomass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2.1. Mean estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.1.2.1.1. Annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate annual level data by:\n",
    "# 1. grouping over source, region and landcover and taking the mean (mean across al methods in each source)\n",
    "# 2. grouping over region and landcover and taking the mean (mean across all sources in each region)\n",
    "# 3. grouping over region and taking the sum (sum across all landcovers in each region)\n",
    "delta_B_obs_annual_mean = biomass_data_obs.groupby(['source','region','landcover']).mean()\\\n",
    "                                            .groupby(['region','landcover']).mean()\\\n",
    "                                            .groupby(['region']).sum()\\\n",
    "                                            .loc[:,start_year+1:2019]\n",
    "# set columns name to time\n",
    "delta_B_obs_annual_mean.columns.name = 'time'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.1.1.1.2. Decadal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set time bins\n",
    "time_bins_edges = [start_year,2000,2010,2019]\n",
    "\n",
    "# copy data from the annual mean data\n",
    "delta_B_obs = calc_regional_stat(delta_B_obs_annual_mean,time_bins_edges,'mean',['region'])\n",
    "\n",
    "# for the mean across the entire study period (1992-2019), just take the mean of the annual mean\n",
    "delta_B_obs.loc[:,'-'.join([str(time_bins_edges[0]+1),str(time_bins_edges[-1])])] = delta_B_obs_annual_mean.mean(axis=1)\n",
    "\n",
    "# for the global estimate, sum over all regions\n",
    "delta_B_obs.loc['Global',:] = delta_B_obs.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2.2. Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.1.2.2.1. Uncorrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basic variables that each sample should have are region and landcover\n",
    "bin_vars=['region','landcover']\n",
    "\n",
    "# define the number of samples to take\n",
    "n = 10_000\n",
    "\n",
    "# generate for each time period N random samples of the data\n",
    "trajectories = Parallel(n_jobs=-1)(delayed(create_dist)(biomass_data_obs,x['year'],bin_vars=bin_vars,n_samples=n) for i,x in time_bins.iterrows())\n",
    "\n",
    "# convert the result into a dataframe\n",
    "trajectories = pd.concat([pd.concat(i,axis=1,keys=range(n)) for i in trajectories],keys=time_bins.index)\n",
    "\n",
    "# set the index names\n",
    "trajectories.index.names=['time_bin'] + bin_vars\n",
    "\n",
    "# use the trajectories to calculate the uncertainty of the data\n",
    "delta_B_obs_std = generate_std_df(trajectories,year_bin_freq).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.1.2.2.2. Correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the trajectories so that they are fully correlated in time\n",
    "trajectories_sorted = trajectories.groupby('time_bin').apply(sort_df)\n",
    "\n",
    "# use the trajectories to calculate the uncertainty of the data\n",
    "delta_B_obs_std_cor = generate_std_df(trajectories_sorted,year_bin_freq).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.1.2.2.3. Annual estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean cumulative sum of the annual rate of change\n",
    "delta_B_obs_annual_cum = np.cumsum(delta_B_obs_annual_mean.sum())\n",
    "delta_B_obs_annual_cum.loc[start_year] = 0\n",
    "delta_B_obs_annual_cum.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the basic variables that each sample should have are region and landcover\n",
    "bin_vars=['region','landcover']\n",
    "\n",
    "# define the number of samples to take\n",
    "n = 10_000\n",
    "\n",
    "# generate for each time period N random samples of the data\n",
    "trajectories_cum = Parallel(n_jobs=-1)(delayed(create_dist)(biomass_data_obs,x['year'],bin_vars=bin_vars,n_samples=n,stat='cumsum') for i,x in time_bins.iterrows())\n",
    "\n",
    "# convert the result into a dataframe\n",
    "trajectories_cum = pd.concat([pd.concat(i,axis=1,keys=range(n)) for i in trajectories_cum],keys=time_bins.index)\n",
    "\n",
    "# set the index names\n",
    "trajectories_cum.index.names = ['time_bin','year'] + bin_vars\n",
    "\n",
    "# sum the samples for each year to get global samples\n",
    "trajectories_cum = trajectories_cum.groupby('year').sum()\n",
    "\n",
    "# for the second and third time periods, add the cumulative biomass of the previous period to beginning of the current period\n",
    "for i,(period,row) in enumerate(time_bins[1:].iterrows()):\n",
    "    sy = row['year'][0]\n",
    "    ey = row['year'][-1]\n",
    "    previous_ey = time_bins.iloc[i]['year'][-1]\n",
    "    trajectories_cum.loc[sy:ey] = trajectories_cum.loc[sy:ey] + trajectories_cum.loc[previous_ey]\n",
    "\n",
    "# set the standard deviation and the std of the trajectories\n",
    "delta_B_obs_annual_std_cum = trajectories_cum.std(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2.3. Different starting years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# the basic variables that each sample should have are region and landcover\n",
    "bin_vars=['region','landcover']\n",
    "\n",
    "# define the number of samples to take\n",
    "n = 10_000\n",
    "\n",
    "# define the results\n",
    "results = []\n",
    "\n",
    "# for each year\n",
    "for sy in np.arange(1993,2011):\n",
    "    \n",
    "    # define time bins\n",
    "    if sy < 2001:\n",
    "        tb = pd.DataFrame(pd.Series([np.arange(sy,2001),np.arange(2001,2011),np.arange(2011,2020)]),columns=['year'])\n",
    "    elif sy < 2011:\n",
    "        tb = pd.DataFrame(pd.Series([np.arange(sy,2011),np.arange(2011,2020)]),columns=['year'])\n",
    "\n",
    "    # create time bin names\n",
    "    tb_names = tb.apply(lambda x: '-'.join([str(x.iloc[0].min()),str(x.iloc[0].max())]),axis=1)\n",
    "\n",
    "    # set the index to be the names\n",
    "    tb.index = tb_names\n",
    "\n",
    "    # calculate the frequency of each time bin\n",
    "    yb_freq = (tb.apply(lambda x: len(x['year']),axis=1)/tb.apply(lambda x: len(x['year']),axis=1).sum())\n",
    "\n",
    "    # generate for each time period N random samples of the data\n",
    "    tjs = Parallel(n_jobs=-1)(delayed(create_dist)(biomass_data_obs,x['year'],bin_vars=bin_vars,n_samples=n) for i,x in tb.iterrows())\n",
    "\n",
    "    # convert the result into a dataframe\n",
    "    tjs = pd.concat([pd.concat(i,axis=1,keys=range(n)) for i in tjs],keys=tb.index)\n",
    "\n",
    "    # set the index names\n",
    "    tjs.index.names=['time_bin'] + bin_vars\n",
    "\n",
    "    # append the results\n",
    "    results.append(generate_std_df(tjs,yb_freq).T)\n",
    "\n",
    "# calculate the mean and std of the global estimates\n",
    "res = pd.DataFrame(np.nan,index=np.arange(1993,2011),columns=['mean_rate','std'])\n",
    "different_starts = pd.DataFrame(index=np.arange(1993,2011),columns=['mean','std'])\n",
    "different_starts['std'] = [r.loc['Global','1993-2019'] for r in results]\n",
    "different_starts['mean'] = (res.apply(lambda x: delta_B_obs_annual_mean.sum().loc[x.name:].mean(),axis=1))\n",
    "\n",
    "different_starts.to_csv('../results/06_make_estimates/delta_B_obs_different_starts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the delta_C for different starting years\n",
    "delta_C_different_start = pd.DataFrame(np.nan,index=np.arange(1993,2011),columns=['mean_rate','std'])\n",
    "delta_C_different_start = delta_C_different_start.apply(lambda x: pd.Series([GCB_data['land_sink'].loc[x.name:2019].mean()-F_lat,calc_land_std(GCB_data,x.name+1,2019,ocean_random,ocean_sys)],index=['mean','std']),axis=1)\n",
    "\n",
    "delta_C_different_start.to_csv('../results/06_make_estimates/delta_C_obs_different_starts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3. Infer changes in non-living carbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean estimate\n",
    "delta_OC_obs = delta_C_obs-delta_B_obs\n",
    "\n",
    "# calculate the uncertainty estimate by propagating the uncertainties of the total carbon stock and living biomass estimates\n",
    "delta_OC_obs_std = np.sqrt(delta_C_obs_std**2+delta_B_obs_std**2)\n",
    "delta_OC_obs_std_cor = np.sqrt(delta_C_obs_std**2+delta_B_obs_std_cor**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4. Save estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_mean = pd.concat([delta_B_obs.stack(),delta_OC_obs.stack(),delta_C_obs.stack()],keys=['delta_B','delta_OC','delta_C'])\n",
    "\n",
    "# method 1 uncorrelated uncertainty\n",
    "obs_std = pd.concat([delta_B_obs_std.stack(),delta_OC_obs_std.stack(),delta_C_obs_std.stack()],keys=['delta_B','delta_OC','delta_C'])\n",
    "obs_periods = pd.concat([obs_mean,obs_std],keys=['mean','std'])\n",
    "obs_periods.index.names = ['stat','pool','time','period']\n",
    "obs_periods.to_csv('../results/06_make_estimates/obs_periods.csv')\n",
    "\n",
    "# method 1 correlated uncertainty\n",
    "obs_std_cor = pd.concat([delta_B_obs_std_cor.stack(),delta_B_obs_std_cor.stack(),delta_C_obs_std.stack()],keys=['delta_B','delta_OC','delta_C'])\n",
    "obs_periods_cor = pd.concat([obs_mean,obs_std_cor],keys=['mean','std'])\n",
    "obs_periods_cor.index.names = ['stat','pool','time','period']\n",
    "obs_periods_cor.to_csv('../results/06_make_estimates/obs_periods_cor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual estimates\n",
    "obs_annual_cum = pd.concat([delta_B_obs_annual_cum,delta_C_obs_annual_cum],keys=['delta_B','delta_C'])\n",
    "obs_annual_cum_std = pd.concat([delta_B_obs_annual_std_cum,delta_C_obs_annual_std_cum],keys=['delta_B','delta_C'])\n",
    "obs_annual_cum = pd.concat([obs_annual_cum,obs_annual_cum_std],keys=['mean','std'])\n",
    "obs_annual_cum.index.names = ['stat','pool','time']\n",
    "obs_annual_cum.to_csv('../results/06_make_estimates/obs_annual_cum.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. DGVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cumsum(df):\n",
    "    # set the first year to be 1992 with a value of zero\n",
    "    res = df.copy()\n",
    "    res[start_year] = 0 \n",
    "    res = res[res.columns.sort_values()]\n",
    "    \n",
    "    # calculate the cumulative sum and then the mean and std across the models\n",
    "    return res.cumsum(axis=1).agg(['mean','std']).T\n",
    "\n",
    "# calculate mean cumsums    \n",
    "delta_B_DGVMs_annual_cum = calc_cumsum(trendy_global_cVeg)['mean']\n",
    "delta_C_DGVMs_annual_cum = calc_cumsum(trendy_global_nbp)['mean']\n",
    "\n",
    "# calculate the std of the cumsums\n",
    "delta_B_DGVMs_annual_std_cum = calc_cumsum(trendy_global_cVeg)['std']\n",
    "delta_C_DGVMs_annual_std_cum = calc_cumsum(trendy_global_nbp)['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Decadal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bins_edges = [start_year,2000,2010,2019]\n",
    "\n",
    "# calculate mean for total carbon stocks and living biomass\n",
    "delta_B_DGVMs = analyze_DGVM(trendy_cVeg_regional,trendy_global_cVeg,time_bins_edges,'mean')\n",
    "delta_C_DGVMs = analyze_DGVM(trendy_nbp_regional,trendy_global_nbp,time_bins_edges,'mean')\n",
    "\n",
    "# calculate std for total carbon stocks and living biomass\n",
    "delta_B_DGVMs_std = analyze_DGVM(trendy_cVeg_regional,trendy_global_cVeg,time_bins_edges,'std')\n",
    "delta_C_DGVMs_std = analyze_DGVM(trendy_nbp_regional,trendy_global_nbp,time_bins_edges,'std')\n",
    "\n",
    "\n",
    "# define the regional and global non-living carbon data\n",
    "trendy_OC_global = (trendy_global_nbp-trendy_global_cVeg).dropna()\n",
    "trendy_OC_regional = trendy_nbp_regional-trendy_cVeg_regional\n",
    "\n",
    "# remove models with missing data\n",
    "trendy_OC_regional = trendy_OC_regional.loc[pd.IndexSlice[:,trendy_OC_global.index],:]\n",
    "\n",
    "# calculate the mean for non-living orgnic carbon\n",
    "delta_OC_DGVMs = delta_C_DGVMs-delta_B_DGVMs\n",
    "\n",
    "# calculate the std for non-living orgnic carbon\n",
    "delta_OC_DGVMs_std = analyze_DGVM(trendy_OC_regional,trendy_OC_global,time_bins_edges,'std')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Single model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the cVeg and nbp data\n",
    "single_model_data = pd.concat([trendy_global_cVeg,trendy_global_nbp],keys=['delta_B','delta_C'],names=['pool'])\n",
    "\n",
    "# copy the DataFrame to calculate the whole period stats\n",
    "whole_period = single_model_data.copy()\n",
    "\n",
    "res = []\n",
    "\n",
    "for df,bins, bin_name in zip([single_model_data,whole_period],[time_bins_edges,[time_bins_edges[0],time_bins_edges[-1]]],[time_bin_names,[f'{start_year+1}-2019']]):\n",
    "    \n",
    "    # cut the columns into the different periods\n",
    "    df.columns = pd.cut(df.columns,bins=bins,labels=bin_name)\n",
    "    df.columns.name = 'period'\n",
    "\n",
    "    # groupby pool, model and period and calculate the mean and std\n",
    "    res.append(df.stack().groupby(['pool','model','period'],observed=False).agg(['mean','std']))\n",
    "\n",
    "# concatenate the results\n",
    "single_model_data = pd.concat(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4. Save estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DGVMs_mean = pd.concat([delta_B_DGVMs.stack(),delta_OC_DGVMs.stack(),delta_C_DGVMs.stack()],keys=['delta_B','delta_OC','delta_C'])\n",
    "\n",
    "# method 1 uncorrelated uncertainty\n",
    "DGVMs_std = pd.concat([delta_B_DGVMs_std.stack(),delta_OC_DGVMs_std.stack(),delta_C_DGVMs_std.stack()],keys=['delta_B','delta_OC','delta_C'])\n",
    "DGVMs_periods = pd.concat([DGVMs_mean,DGVMs_std],keys=['mean','std'])\n",
    "DGVMs_periods.index.names = ['stat','pool','time','period']\n",
    "DGVMs_periods.to_csv('../results/06_make_estimates/DGVMs_periods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DGVMs_annual_cum = pd.concat([delta_B_DGVMs_annual_cum,delta_C_DGVMs_annual_cum],keys=['delta_B','delta_C'])\n",
    "DGVMs_annual_std_cum = pd.concat([delta_B_DGVMs_annual_std_cum,delta_C_DGVMs_annual_std_cum],keys=['delta_B','delta_C'])\n",
    "DGVMs_annual_cum     = pd.concat([DGVMs_annual_cum,DGVMs_annual_std_cum],keys=['mean','std'])\n",
    "DGVMs_annual_cum.index.names = ['stat','pool','time']\n",
    "DGVMs_annual_cum.to_csv('../results/06_make_estimates/DGVMs_annual_cum.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_model_data.to_csv('../results/06_make_estimates/single_DGVM_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
