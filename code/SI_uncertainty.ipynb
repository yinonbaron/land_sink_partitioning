{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Analysis - Quantifying the sources of uncertainty to our final predictions of biomass stock gains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sources of uncertainty we consider here are:\n",
    "\n",
    "1. Uncertainty associated with each data source (we assume it to be 30% CV based on [Yang et al. (2023)](https://www.nature.com/articles/s41561-023-01274-4), [Pan et al. (2024)](https://www.nature.com/articles/s41586-024-07602-x) and [Xu et al. (2021)](https://www.science.org/doi/10.1126/sciadv.abe9829))\n",
    "2. Variation between data sources\n",
    "3. Variation across different analysis pipelines (e.g. different methods for splitting forest and non-froest areas, above to below ground biomass ratios, etc.)\n",
    "4. Variation associated with different assumptions about undetectable growth in mature forests (e.g. no such effect, an effect in forests with >100 MgC ha<sup>-1</sup> biomass stocks or an effect in forests with >200 MgC ha<sup>-1</sup> biomass stocks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "biomass_data_obs = pd.read_csv('../results/04_temporal_harmonization/harmonized_biomass_data.csv',index_col=[0,1,2,3])\n",
    "biomass_data_obs.columns = biomass_data_obs.columns.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define functions for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the starting year of the analysis\n",
    "start_year = 1992\n",
    "\n",
    "# the number of bootstrap samples\n",
    "n = 10_000\n",
    "\n",
    "# define time bins\n",
    "time_bins = pd.DataFrame(pd.Series([np.arange(start_year+1,2001),np.arange(2001,2011),np.arange(2011,2020)]),columns=['year'])\n",
    "\n",
    "# create time bin names\n",
    "time_bin_names = time_bins.apply(lambda x: '-'.join([str(x.iloc[0].min()),str(x.iloc[0].max())]),axis=1)\n",
    "\n",
    "# set the index to be the names\n",
    "time_bins.index = time_bin_names\n",
    "\n",
    "# calculate the frequency of each time bin\n",
    "year_bin_freq = (time_bins.apply(lambda x: len(x['year']),axis=1)/time_bins.apply(lambda x: len(x['year']),axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_uncertainty(df:pd.DataFrame,time_bin:np.array,bin_vars:list,utype:str,n_samples=10000) -> list:\n",
    "    \"\"\"\n",
    "    Create random samples from the data for each time bin, considering only one source of uncertainty. \n",
    "\n",
    "    Parameters:\n",
    "    df: pd.DataFrame\n",
    "        The data frame with the data\n",
    "    time_bin: np.array\n",
    "        The time bin to sample from\n",
    "    bin_vars: list\n",
    "        The variables to use as columns\n",
    "    utype: str\n",
    "        The type of uncertainty to be considered - must be one of 'intra', 'pipeline', 'inter' or 'dense_forests'\n",
    "    n_samples: int\n",
    "        The number of samples to create\n",
    "    \n",
    "    Returns:\n",
    "    list\n",
    "        A list with the samples\n",
    "    \"\"\"\n",
    "\n",
    "    # check if the type is valid\n",
    "    assert utype in ['intra','pipeline','inter','dense_forests'], \"type must be one of 'intra','pipeline','inter' or 'dense_forests'\"\n",
    "\n",
    "    # select the data that for the years in time_bin, drop missing values and calculate make the bin_vars as columns\n",
    "    df_tb = df.loc[:,df.columns.isin(time_bin)].mean(axis=1).dropna().unstack(bin_vars)\n",
    "\n",
    "    # define the result list\n",
    "    res = []\n",
    "\n",
    "    if utype == 'intra':\n",
    "        # for each sample, calculate the mean rate of change across data sources and their variants, and sample from a 30% \n",
    "        # coefficient of variation for each estimate at the regional scale.\n",
    "        \n",
    "        # calculate the mean rate of change for each data source across their variants\n",
    "        df = df_tb.groupby('source').mean()\n",
    "\n",
    "        # # add random noise to the data with a certain coefficient of variation\n",
    "        cv = 0.3 # typical number from the literature\n",
    "        samples = np.stack([df+np.random.normal(0,cv*df.abs()) for i in range(n_samples)])\n",
    "        \n",
    "        # calculate the mean rate of change across data sources and their variants, and sum across bin_vars\n",
    "        res = np.nanmean(samples,axis=1).sum(axis=1)\n",
    "    \n",
    "    elif utype == 'inter':\n",
    "\n",
    "        # for each sample, calculate the mean rate of change for each data sources across their variants\n",
    "        # then sample from one data source randomly. If that data source doesn't cover all regions and landcovers,\n",
    "        # sample from another data source and fill the missing values with the values from the second data source.\n",
    "\n",
    "        # for each sample\n",
    "        for i in range(n_samples):\n",
    "            \n",
    "            # calculate the mean rate of change for each data source across their variants, \n",
    "            # then sample one data source randomly\n",
    "            sample = df_tb.groupby('source').mean().sample(1).T.iloc[:,0]\n",
    "\n",
    "            # while the sample doesn't cover all regions and landcovers (has nan values)\n",
    "            while sample.isna().any():\n",
    "                # sample again\n",
    "                sample2 = df_tb.groupby('source').mean().sample(1).T.iloc[:,0]\n",
    "                \n",
    "                # merge the two samples\n",
    "                merged_samples = pd.concat([sample,sample2],axis=1,keys=['first_sample','second_sample'])\n",
    "\n",
    "                # fill the missing values in the original sample with the values from the second sample\n",
    "                sample = merged_samples['first_sample'].fillna(merged_samples['second_sample'])\n",
    "                    \n",
    "            # add the sample to the result list\n",
    "            res.append(sample)\n",
    "\n",
    "\n",
    "    elif utype in ['pipeline','dense_forests']:\n",
    "        # for each sample, calculate the mean rate of change across data sources and their variants, then sample \n",
    "        # one variant for each data source randomly.\n",
    "\n",
    "        if utype == 'dense_forests':\n",
    "            variant = df_tb.index.get_level_values('method').str.split('TB_').str[1].str.split(\"_regions\").str[0]\n",
    "        else:\n",
    "            variant = df_tb.index.get_level_values('method').str.split('biomass_').str[1].str.split(\"_TB\").str[0]\n",
    "\n",
    "        # add the variant to the data frame\n",
    "        df_tb['variant'] = variant\n",
    "\n",
    "        # add the variant to the indices of the data frame\n",
    "        df_tb.set_index('variant',append=True,inplace=True)\n",
    "\n",
    "        # for each sample\n",
    "        for i in range(n_samples):\n",
    "                            \n",
    "            # add the sample to the result list\n",
    "            res.append(df_tb.groupby(['source','variant']).mean().groupby('source').sample(1).mean().sum())\n",
    "                \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Intra-data source uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basic variables that each sample should have are region and landcover\n",
    "bin_vars=['region','landcover']\n",
    "\n",
    "# generate for each time period N random samples of the data\n",
    "trajectories = Parallel(n_jobs=-1)(delayed(sample_uncertainty)(biomass_data_obs,x['year'],bin_vars=bin_vars,utype='intra',n_samples=n) for i,x in time_bins.iterrows())\n",
    "\n",
    "# calculate the uncertainty of the estimate across time bins\n",
    "intra_sd = (np.stack(trajectories).T @ year_bin_freq).std()/1e15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Inter-data source uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate for each time period N random samples of the data\n",
    "trajectories = Parallel(n_jobs=-1)(delayed(sample_uncertainty)(biomass_data_obs,x['year'],bin_vars=bin_vars,utype='inter',n_samples=n) for i,x in time_bins.iterrows())\n",
    "trajectories_df = pd.concat([pd.concat(i,axis=1,keys=range(n)) for i in trajectories],keys=time_bins.index)\n",
    "trajectories_df.index.names = ['time_bin'] + bin_vars\n",
    "\n",
    "inter_source_sd = (trajectories_df.groupby('time_bin').sum().T @ year_bin_freq).std()/1e15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Inter-analysis pipeline uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate for each time period N random samples of the data\n",
    "trajectories = Parallel(n_jobs=-1)(delayed(sample_uncertainty)(biomass_data_obs,x['year'],bin_vars=bin_vars,utype='pipeline',n_samples=n) for i,x in time_bins.iterrows())\n",
    "\n",
    "pipeline_sd = (np.array(trajectories).T @ year_bin_freq).std()/1e15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Uncertainty associated with undetectable growth in mature forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate for each time period N random samples of the data\n",
    "trajectories = Parallel(n_jobs=-1)(delayed(sample_uncertainty)(biomass_data_obs,x['year'],bin_vars=bin_vars,utype='dense_forests',n_samples=n) for i,x in time_bins.iterrows())\n",
    "\n",
    "mature_forest_sd = (np.array(trajectories).T @ year_bin_freq).std()/1e15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intra_source     0.046135\n",
       "inter_source     0.207648\n",
       "pipeline         0.006185\n",
       "mature_forest    0.078274\n",
       "dtype: float64"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertainty_df = pd.Series([intra_sd,inter_source_sd,pipeline_sd,mature_forest_sd],index=['intra_source','inter_source','pipeline','mature_forest'])\n",
    "uncertainty_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
