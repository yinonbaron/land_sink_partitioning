{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmonize data sources in time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "We convert carbon stocks data into carbon stock changes per year. We do that both for the living biomass observational data and for the DGVM cVeg data.\n",
    "\n",
    "For the living biomass observation data, we have two aspects of the data that we need to harmonize in time:\n",
    "\n",
    "1. Some data sources do not have annual data. We linearly interpolate the data between missing years\n",
    "\n",
    "2. Besnard et al. reports the data in units of biomass change per year, where as all other sources report stocks per time period. We convert all stock data into stock changes per year.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Biomass observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "files = glob('../results/03_aggregate_regions/*regions.csv')\n",
    "\n",
    "# extract file names\n",
    "file_names = np.array([x.split('/')[-1].split('.')[0] for x in files])\n",
    "\n",
    "# extract the data source from the file name\n",
    "sources = [x.split('_')[0] for x in file_names]\n",
    "\n",
    "# change LVODmin and LVODmax to LVOD\n",
    "sources = ['LVOD' if x in ['LVODmin','LVODmax'] else x for x in sources]\n",
    "\n",
    "# extract the method from the file name\n",
    "method = ['_'.join(x.split('_')[:-1]) for x in file_names]\n",
    "\n",
    "# create a file metadata dataframe\n",
    "file_metadata = pd.DataFrame(np.array([files, sources,file_names]).T,columns=['file','source','method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(fn):\n",
    "    # define a function to process files\n",
    "\n",
    "    # load file and set two first columns as indices\n",
    "    df  = pd.read_csv(fn,index_col=[0,1])\n",
    "\n",
    "    # set the columns as integers\n",
    "    df.columns = df.columns.astype(float).astype(int) \n",
    "\n",
    "    # set the names of the indices\n",
    "    df.index.names = ['RECCAP reg','landcover']\n",
    "\n",
    "    # add the file name to the indices\n",
    "    ind = df.reset_index()[['RECCAP reg','landcover']]\n",
    "    ind['file'] = fn\n",
    "    df.index = pd.MultiIndex.from_frame(ind[['file','RECCAP reg','landcover']])\n",
    "\n",
    "    return df\n",
    "\n",
    "# process the files\n",
    "merged_data = pd.concat([process_file(row['file']) for i,row in file_metadata.iterrows()])\n",
    "\n",
    "# merge the data with the metadata to add source to each row\n",
    "merged_data = merged_data.reset_index().merge(file_metadata, on='file').set_index(['source','method','RECCAP reg','landcover']).drop(columns='file')    \n",
    "\n",
    "# sort data such that columns are in ascending order\n",
    "merged_data = merged_data[merged_data.columns.sort_values()]\n",
    "\n",
    "# interpolate missing years\n",
    "merged_data = merged_data.interpolate(axis=1,limit_area='inside')\n",
    "\n",
    "# get the Besnard et al. data\n",
    "besnard_data = merged_data.loc[pd.IndexSlice['besnard',:,:]]\n",
    "\n",
    "# apply a time derivative to all the data\n",
    "merged_data = merged_data.diff(axis=1)\n",
    "\n",
    "# reset the Besnard et al. values\n",
    "merged_data.loc[pd.IndexSlice['besnard',:,:]] = besnard_data.values\n",
    "\n",
    "# remove blank columns\n",
    "merged_data = merged_data.loc[:,merged_data.sum() !=0]\n",
    "\n",
    "# rename the indices\n",
    "merged_data.index.names = ['source','method','region','landcover']\n",
    "\n",
    "# save the results\n",
    "merged_data.to_csv('../results/04_temporal_harmonization/harmonized_biomass_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. DGVM cVeg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trendy_cVeg_regional = pd.read_csv('../results/03_aggregate_regions/regional_DGVM_cVeg.csv',index_col=[0,1])\n",
    "\n",
    "# differentiate in time\n",
    "trendy_cVeg_regional = trendy_cVeg_regional.diff(axis=1).dropna(axis=1)\n",
    "\n",
    "# save the results\n",
    "trendy_cVeg_regional.to_csv('../results/04_temporal_harmonization/harmonized_DGVM_cVeg.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
