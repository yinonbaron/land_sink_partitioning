{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import rioxarray as rio\n",
    "from glob import glob\n",
    "from utils import *\n",
    "from geocube.api.core import make_geocube\n",
    "from typing import Union\n",
    "from types import NoneType\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from rasterio.enums import Resampling\n",
    "from tqdm.contrib import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the fraction of forest biomass out of global biomass\n",
    "\n",
    "## 1. Introduction\n",
    "This procedure is intended to split the spatially-explicit biomass estimates into the forest fraction and the non-forest fractions. The reason a split into forest and non-forest biomass is needed is to incorporate into our final estimate some of the estimates that we have (namely Pan et al. and the FRA) that cover only on forest biomass. We thus use the FRA definition of a forest which is an area greater than 0.5 ha that has more than 10% coverage of trees that are above 5 meters tall.\n",
    "\n",
    "\n",
    "The two main ingredients we need in order to split the biomass maps into forest and non-forest biomass are:\n",
    "\n",
    "1. Maps of forest cover \n",
    "\n",
    "2. Estimates of the biomass density of forest and non-forest lands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Maps of forest cover\n",
    "For the forest cover maps, we use the ESA CCI land cover maps. We relay on these maps because they have annual data with global coverage at relative high resolution (300m). We map the native ESA CCI land cover types into forest and non-forest land covers (shurbland, cropland/grassland, and bare ground) based on Table S1 in [Tagesson et al. 2020](https://doi.org/10.1038/s41559-019-1090-0). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Estimate forest and nonforest biomass density\n",
    "\n",
    "We generate several estimates of the biomass density in forests and nonforests in descrete regions around the world. Generally the estimates are based on either of two approaches:\n",
    "1. Estimates from the literature\n",
    "2. A spatial regression approach\n",
    "\n",
    "For the literature estimates, we use the estimates from [Xu et al. 2021](https://www.science.org/doi/10.1126/sciadv.abe9829) and the regions defined therein.\n",
    "\n",
    "For the regression method, we rely on the two following datasets and use [WWF regions](https://en.wikipedia.org/wiki/List_of_terrestrial_ecoregions_(WWF)) as regions: \n",
    "1. [Song et al. 2016](https://www.nature.com/articles/s41586-018-0411-9)\n",
    "2. ESA CCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Estimating forest biomass fraction\n",
    "\n",
    "All of our biomass data sources the cover both forest and non-forest biomass are spatially explicit except [Besnard et al.](https://onlinelibrary.wiley.com/doi/full/10.1111/gcb.15877), which reports values for Global Fire Emissions Database (GFED) regions.\n",
    "\n",
    "#### Spatially explicit biomass estimates\n",
    "\n",
    "For each pixel for each year in each biomass data source, we calculate the fraction of the biomass found in forests. To do that, we use the following equation:\n",
    "\n",
    "$$ F(t)_i^{forest} = \\frac{A(t)_i^{forest} \\times D(t)_i^{forest}}{A(t)_i^{forest} \\times D(t)_i^{forest} +A(t)_i^{nonforest} \\times D(t)_i^{nonforest}} $$\n",
    "\n",
    "where $F(t)_i^{forest}$ is the fraction of the biomass in pixel i found in forests at time t, $A(t)_i^{forest}$ is the area of forest in pixel i at time t, $D(t)_i^{forest}$ is the biomass density of forest in pixel i at time t, $A(t)_i^{nonforest}$ is the area of non-forest in pixel i at time t, and $D(t)_i^{nonforest}$ is the biomass density of non-forest in pixel i at time t.\n",
    "\n",
    "As stated above the source for $A(t)_i^{forest}$ is the ESA CCI land cover maps, and $A(t)_i^{nonforest}$ is the difference between the total pixel area and $A(t)_i^{forest}$.\n",
    "\n",
    "$D(t)_i^{forest}$ and $D(t)_i^{nonforest}$ are determined based on descrete regions, and are determined either from [Xu et al. 2021](https://www.science.org/doi/10.1126/sciadv.abe9829) or the regression estimates.\n",
    "\n",
    "For the regression approach, we estimate the forest and nonforest biomass densities in each region for each one of the biomass data sources by pooling all of the pixels for the data sources in the region, and using a linear regression model in which we regress the total biomass density of each pixel (in units of $MgC\\ ha^{-1}$) as a function forest and nonforest area fractions and the regional forest and nonforest biomass densities. Our regression follows the following equation:\n",
    "$$B_i(t) = F(t)_i^{forest} \\times D(t)_r^{forest} + F(t)_i^{nonforest} \\times D(t)_r^{nonforest} + \\epsilon_i$$\n",
    "\n",
    "where $B_i(t)$ is the biomass density of pixel i at time t, $F(t)_i^{forest}$ and $F(t)_i^{nonforest}$ are the fractions of forest and nonforest area in pixel i at time t, $D(t)_r^{forest}$ and $D(t)_r^{nonforest}$ are the forest and nonforest biomass densities in region r at time t, and $\\epsilon_i$ is the error term.\n",
    "\n",
    "As stated above, the data sources we use for $F(t)_i^{forest}$ and $F(t)_i^{nonforest}$ are the ESA CCI land cover dataset and [Song et al. 2016](https://www.nature.com/articles/s41586-018-0411-9) and the regions are defined by the WWF ecoregions.\n",
    "\n",
    "#### Non-spatially explicit biomass estimates\n",
    "\n",
    "The [Besnard et al.](https://onlinelibrary.wiley.com/doi/full/10.1111/gcb.15877) dataset does not report spatially-resolved maps of global biomass for each year. To split we rely #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define functions for the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. For gridded data sources\n",
    "\n",
    "We define four functions used for the analysis:\n",
    "\n",
    "1. `split_biomass`: This function splits the biomass data into forest and non-forest biomass based on the area fraction of each type and the biomass densities of each type. To calculate the biomass densities of each type, it calls the `get_biomass_density` function.\n",
    "\n",
    "2. `get_biomass_density`: This function calculates the biomass densities of the different landcover types for each region. It can perform this calculation in three different methods as stated above. For the regression-based approaches, it calls the `regress_density` function.\n",
    "\n",
    "3. `regress_density`: This function performs the regression analysis to estimate the biomass densities of the different landcover types for each region. It iterates over each region and calls the `regress_density_region` function.\n",
    "\n",
    "4. `regress_density_region`: This function performs the regression analysis to estimate the biomass densities of the different landcover types for a single region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_density_region(ds:xr.DataArray) -> xr.DataArray:\n",
    "    '''\n",
    "    This function calculates the coefficients of Linear Regression for Land Cover Biomass against Land Cover categories.\n",
    "\n",
    "    Parameters:\n",
    "    ds: xarray.DataArray\n",
    "        The DataArray containing the landcover and biomass data.\n",
    "\n",
    "    Returns:\n",
    "    xarray.DataArray\n",
    "        The coefficients of Linear Regression for Land Cover Biomass against Land Cover categories.\n",
    "    '''\n",
    "\n",
    "    # reshape the input data into a 2D array where rows represent each sample and columns represent individual features.\n",
    "    X = ds.values.reshape((ds.shape[0],-1))\n",
    "\n",
    "    # extract and flatten the 'biomass' column from the input data, and remove NaN values.\n",
    "    y = ds['biomass'].values.flatten()\n",
    "    mask = (~np.isnan(X.sum(axis=0))) & ~np.isnan(y).squeeze() # Squeeze the boolean value into a scalar.\n",
    "    \n",
    "    # mask out columns from X that have one or more NaN values, update output variables.\n",
    "    X = X[:,mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    # if the filtered inputs have an N of more than 500 valid features, run Linear Regression to estimate coefficients, else return an empty data array.\n",
    "    if X.shape[1]>=500:\n",
    "        # implement Linear Regression with positive constraint on regression weights using the filtered inputs.\n",
    "        reg = LinearRegression(positive=True,fit_intercept=False).fit(X.T, y)\n",
    "    \n",
    "        # return a DataArray containing the learned coefficients with dimensions 'landcover'.\n",
    "        return xr.DataArray(reg.coef_,dims=['landcover'])\n",
    "    else:\n",
    "        # create and return a DataArray with shape 'ds.shape[0]' and all elements initialized with NaN values.\n",
    "        return xr.DataArray(np.full(ds.shape[0],np.nan),dims=['landcover'])\n",
    "\n",
    "\n",
    "def regress_density(biomass:xr.DataArray, area_map:xr.DataArray,regions:xr.DataArray) -> np.ndarray:\n",
    "    '''\n",
    "    This function calculates the biomass densities for each region based on the regression method.\n",
    "\n",
    "    Parameters:\n",
    "    biomass: xarray.DataArray\n",
    "        The biomass data to be split.\n",
    "    area_map: xarray.DataArray\n",
    "        The area map used to define the landcover types.\n",
    "    regions: xarray.DataArray\n",
    "        The regions used to define different biomass densities for each landcover.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray\n",
    "        The biomass densities for each region for the different landcovers.\n",
    "    '''\n",
    "\n",
    "    # create a merged DataArray with the landcover map, the biomass map and the region map\n",
    "    merged_ds = area_map\n",
    "    merged_ds['biomass'] = biomass\n",
    "    merged_ds['area_id'] = regions\n",
    "\n",
    "    # group the DataArray by region and apply the regression function\n",
    "    density = merged_ds.groupby('area_id').apply(regress_density_region)\n",
    "\n",
    "    # convert the result into an ndarray\n",
    "    density = density.to_dataframe(name='').unstack().T.droplevel(0).reindex(list(range(0,int(merged_ds['area_id'].max().values)+1)),fill_value=np.nan)\n",
    "    \n",
    "    return density.values\n",
    "\n",
    "def get_biomass_density(biomass:Union[xr.DataArray,NoneType],density_args:Union[xr.DataArray,NoneType],regions:xr.DataArray,method:str) -> np.ndarray:\n",
    "    '''\n",
    "    This function calculates the biomass densities for each region based on the method used.\n",
    "\n",
    "    Parameters:\n",
    "    biomass: Union[xr.DataArray,NoneType]\n",
    "        The biomass data to be split. If using Xu method, this parameter is not used.\n",
    "    density_args: Union[xarray.DataArray,NoneType]\n",
    "        Arguments for calculating the density (area_map for the regression method).\n",
    "    regions: xarray.DataArray\n",
    "        The regions used to define different forest/nonforest biomass densities.\n",
    "    method: str\n",
    "        The method to use for biomass density calculation. Options are 'xu' and 'song' or 'CCI'.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray\n",
    "        The biomass densities for each region.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    assert method in ['xu','song','CCI'], 'Invalid method. Options are \"xu\", \"song\" or \"CCI\".'\n",
    "    \n",
    "    if method == 'xu':\n",
    "        # load Table 1 from Xu et al.\n",
    "        xu_biomass_density_df = pd.read_excel('../data/regions_data/xu_et_al_2021/biomass_densities_table1.xlsx')\n",
    "\n",
    "        # take the columns that report biomass densities\n",
    "        densities = xu_biomass_density_df.filter(regex='Mg/ha')\n",
    "\n",
    "        # calculate the average biomass density for 2000 and 2019\n",
    "        densities = (densities.filter(regex='2000').values + densities.filter(regex='2019').values) / 2\n",
    "        \n",
    "        # change the column names to forest and shrub\n",
    "        xu_biomass_density_df[['forest','shrubland']] = densities\n",
    "\n",
    "        # set the biomass density to 0 for cropland\n",
    "        xu_biomass_density_df['cropland'] = 0\n",
    "\n",
    "        # get the biomass densities and the code for the regions\n",
    "        densities_per_area = xu_biomass_density_df[['code','forest','shrubland','cropland']].set_index('code').sort_index().values\n",
    "\n",
    "    if method == 'song':\n",
    "        # for song et al., calculate density based on average biomass density and average landcover for \n",
    "        # the entire period due to non-smooth behavior of the landcover map in time\n",
    "        densities_per_area = regress_density(biomass.mean(dim='time'),density_args.mean(dim='time'),regions)\n",
    "    \n",
    "    if method == 'CCI':\n",
    "        # regress the biomass densities for each region for each year in biomass\n",
    "        densities_per_area = np.stack([regress_density(biomass.sel(time=year),density_args.sel(time=year),regions) for year in biomass['time']])\n",
    "\n",
    "    return densities_per_area\n",
    "    \n",
    "\n",
    "def split_biomass(biomass:xr.DataArray,area_frac:xr.DataArray,density_args:Union[xr.DataArray,NoneType], regions:Union[gpd.GeoDataFrame,xr.DataArray], method:str,test=False) -> xr.DataArray:\n",
    "    '''\n",
    "    This function splits the biomass data into forest and non-forest biomass based on the landcover map and the area fraction of each landcover type.\n",
    "\n",
    "    The stages of the analysis are:\n",
    "    - Resample the area fraction data to match the biomass data resolution.\n",
    "    - Calculate the biomass densities based on the method used.\n",
    "    - Split the biomass data into forest and non-forest biomass based on the landcover map and the area fraction of each landcover type.\n",
    "\n",
    "    Parameters:\n",
    "    biomass: xarray.DataArray\n",
    "        The biomass data to be split.\n",
    "    area_frac: xarray.DataArray\n",
    "        The area fraction of each landcover type.\n",
    "    density_args: Union[xarray.DataArray,NoneType]\n",
    "        Arguments for calculating the density (area_map for the regression method).\n",
    "    regions: xarray.DataArray or geopandas.GeoDataFrame\n",
    "        The regions used to define different forest/nonforest biomass densities.\n",
    "    method: str\n",
    "        The method to use for biomass density calculation. Options are 'xu' and 'song' or 'CCI'.\n",
    "    test: bool\n",
    "        If True, the function will output the data to allow comparing it with the original data. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    xarray.DataArray\n",
    "        The split biomass data.\n",
    "    '''\n",
    "    \n",
    "    assert method in ['xu','song','CCI'], 'Invalid method. Options are \"xu\", \"song\" or \"CCI\".'\n",
    "\n",
    "    ## 1. Resampling of input data ##\n",
    "\n",
    "    \n",
    "    # select the time period of the area fraction data that matches the biomass data\n",
    "    area_frac = area_frac.sel(time = biomass.time)\n",
    "    \n",
    "    # resample the area fraction data to match the biomass data resolution\n",
    "    area_frac = xr.concat([resample_match(area_frac.sel(landcover=i),biomass) for i in area_frac['landcover']],dim='landcover')\n",
    "\n",
    "    # remove artifacts from the resampling\n",
    "    area_frac = area_frac.where(area_frac<1e30)\n",
    "\n",
    "    if method != 'xu':\n",
    "        # for both regression based approaches ('song' or 'CCI'), the regions are the wwf ecoregions. We convert the GeoDataFrame to a xr.DataArray\n",
    "        regions = make_geocube(vector_data=regions,measurements=['id'],like= biomass)['id']\n",
    "\n",
    "        # for the regression based approaches, we need a forest area map ('song' or 'CCI'), so we need to resample the area map to match the biomass data resolution\n",
    "        if method == 'CCI':\n",
    "            # avoid resampling twice if CCI is used both for the area and also for the regression\n",
    "            density_args = area_frac\n",
    "        else:\n",
    "            density_args = xr.concat([resample_match(density_args.sel(landcover=i),biomass) for i in density_args['landcover']],dim='landcover')\n",
    "    \n",
    "    # resample regions to match the biomass data resolution\n",
    "    regions = regions.rio.reproject_match(biomass[0,:,:],nodata=np.nan, resampling=Resampling.nearest)\n",
    "\n",
    "    # fill in missing region values based on the nearest valid region\n",
    "    regions = regions.rio.interpolate_na()\n",
    "\n",
    "    ## 2. Calculate biomass densities ##\n",
    "    # calculate the biomass densities per region\n",
    "    densities_per_area = get_biomass_density(biomass,density_args,regions,method=method)\n",
    "    \n",
    "    # create a mask for NaN values for the regions\n",
    "    nan_mask = np.isnan(regions.values)\n",
    "\n",
    "    if len(densities_per_area.shape) == 2:\n",
    "        # if densities do not have a time dimension (xu or song method) add the time dimension\n",
    "        densities_per_area = densities_per_area[np.newaxis,:,:]\n",
    "\n",
    "    # fill in NaN values in the biomass density by using the weighted mean of regions with valid values based on their dominance terms of area\n",
    "    # count the occurance of each region in the biomass data\n",
    "    region_counts = regions.where(biomass.mean(dim='time')>0).to_dataframe('id')['id'].value_counts()\n",
    "\n",
    "    # calculate the weighted average biomass density based on the occurances of each region and their biomass densities\n",
    "    average_densities = np.nansum(densities_per_area[:,region_counts.index.astype(int),:].transpose(0,2,1) * (region_counts/region_counts.sum()).values,axis=2)\n",
    "\n",
    "    # fill in NaN values in the biomass density using the time average on the average_densities\n",
    "    densities_per_area[np.isnan(densities_per_area.sum(axis=2))] = average_densities.mean(axis=0)\n",
    "\n",
    "    #convert the biomass densities per area into maps of biomass density using the region map\n",
    "    density = densities_per_area[:,regions.fillna(0).values.astype(int)]\n",
    "    \n",
    "    # reapply the NaN mask\n",
    "    density[:,nan_mask,:] = np.nan    \n",
    "\n",
    "    # transpose the density data to match the dimensions of the biomass data\n",
    "    density = density.transpose(3,1,2,0)\n",
    "\n",
    "    ## 3. Split biomass data ##\n",
    "\n",
    "    # calculate the fraction of biomass for each landcover type\n",
    "    fraction = density*area_frac.transpose('landcover','y','x','time')\n",
    "    fraction = (fraction/fraction.sum(dim='landcover')).transpose('landcover','time','y','x')\n",
    "\n",
    "    # multiply by the total biomass to get the final answer \n",
    "    result = fraction * biomass\n",
    "\n",
    "    if test:\n",
    "        # if landcover has three values, we need to sum cropland and shrubland into nonforest\n",
    "        if 'cropland' in result['landcover']:\n",
    "            \n",
    "            # first, we test that this version of the code produces results that are close with the original data when summed up across landcovers\n",
    "            nonforest = result.sel(landcover=['cropland','shrubland']).sum(dim='landcover').expand_dims('landcover')\n",
    "        else:\n",
    "            return result\n",
    "    else:\n",
    "        # then we subtitute the code to produce results that sum up exactly to the original data\n",
    "        nonforest = (biomass - result.sel(landcover=['forest']).fillna(0))\n",
    "        \n",
    "    nonforest['landcover'] = ['nonforest']\n",
    "    result = xr.concat([result.sel(landcover=['forest']),nonforest],dim='landcover')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. For regional data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_region_stats(values:xr.DataArray,region:xr.DataArray,gb_dim:list) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function calculates the fraction of each value in the values DataArray for each region in the region DataArray.\n",
    "\n",
    "    Parameters:\n",
    "    values: xarray.DataArray\n",
    "        The values to calculate the regional fraction for.\n",
    "    region: xarray.DataArray\n",
    "        The regions to calculate statistics based on.\n",
    "    gb_dim: list\n",
    "        The dimensions to group by the final result (if you have also time dimension in the data for example).\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame\n",
    "        The fraction of each value in the values DataArray for each region in the region DataArray.\n",
    "    '''\n",
    "\n",
    "    # calculate the total for each value in values by accounting for the area of each grid cell\n",
    "    merged_ds = values*calc_area(values)\n",
    "\n",
    "    # add to the merged DataArray the region DataArray\n",
    "    merged_ds['region'] = region\n",
    "\n",
    "    # group the DataArray by region and sum the values for each region\n",
    "    df = merged_ds.drop_vars('spatial_ref').groupby('region').apply(lambda x: x.sum(dim='stacked_y_x')).to_dataframe(name='area')\n",
    "\n",
    "    # normalize the sum by the total sum for the gb_dim dimensions\n",
    "    df = df.div(df.groupby(gb_dim).sum())\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_biomass_regional(biomass_regions:xr.DataArray, biomass:pd.DataFrame ,metadata:pd.DataFrame, density_regions:xr.DataArray, forest_map:xr.DataArray) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function splits the regional biomass data into forest and non-forest biomass based on the biomass densities defined by Xu et al. (2021).\n",
    "\n",
    "    The analysis has 4 steps:\n",
    "    1. Load and resample inputs - resample the forest area fraction data to match the region_map data resolution. Load Xu et al. biomass density data.\n",
    "    2. Calculate biomass densities and forest/nonforest stats per region - for each biomass_region, calculate area fraction of each density_region and of forest/nonforest area fraction.\n",
    "    3. Calculate average biomass density per biomass_region - calculate the average biomass density per biomass_region by multiplying the area fraction of each density_region by its correspoding biomass density.\n",
    "    4. Calculate biomass fraction per region - calculate the biomass fraction per biomass_region by multiplying the forest/nonforest area fraction by the biomass density\n",
    "\n",
    "    Parameters:\n",
    "    biomass_regions: xarray.DataArray\n",
    "        The regions used to define the biomass data.\n",
    "    biomass: pandas.DataFrame\n",
    "        The regional biomass data.\n",
    "    metadata: pandas.DataFrame\n",
    "        The the names of the biomass regions.\n",
    "    density_regions: xarray.DataArray\n",
    "        The regions used to define different forest/nonforest biomass densities.\n",
    "    forest_map: xarray.DataArray\n",
    "        The forest/nonforest area fraction data.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame\n",
    "        The split biomass data.\n",
    "    '''\n",
    "    \n",
    "    ## 1. Load and resample inputs\n",
    "    \n",
    "    # resample the forest area fraction data to match the region map data resolution\n",
    "    forest_map = xr.concat([resample_match(forest_map.sel(landcover=i),biomass_regions) for i in forest_map['landcover']],dim='landcover')\n",
    "    \n",
    "    # load Xu et al. biomass density data\n",
    "    regional_biomass_density = get_biomass_density(None,None,biomass_regions,method='xu')\n",
    "    regional_biomass_density = pd.DataFrame(regional_biomass_density[:,:-1],index = np.arange(19),columns=['forest','nonforest'])\n",
    "    \n",
    "    ## 2. Calculate biomass densities and forest/nonforest stats per region\n",
    "\n",
    "    # for each biomass_region, calculate area fraction of each density_region in the biomass density data in Xu et al. and for forest/nonforest area fraction\n",
    "    region_biomass_density_area_frac = calc_region_stats(density_regions,biomass_regions,gb_dim='region')\n",
    "    region_f_non_f_frac = calc_region_stats(forest_map,biomass_regions,gb_dim=['region','time'])\n",
    "\n",
    "    ## 3. calculate average biomass density per biomass_region\n",
    "\n",
    "    # merge the density_region area fraction with the biomass density data \n",
    "    region_density = region_biomass_density_area_frac.merge(regional_biomass_density,left_on='region',right_index=True)\n",
    "\n",
    "    # calculate the average biomass density per region by multiplying the area fraction of each density region by the biomass density\n",
    "    region_density = region_density.groupby('region').apply(lambda x: (x[['forest','nonforest']].mul(x['area'],axis=0)).sum())\n",
    "    \n",
    "    # stack the data and rename the columns\n",
    "    region_density.columns.name = 'landcover'\n",
    "    region_density =region_density.stack()\n",
    "    region_density.name='density'\n",
    "\n",
    "    ## 4. calculate biomass fraction per region\n",
    "\n",
    "    # merge the forest/nonforest area fraction with the biomass density data\n",
    "    region_merge = region_f_non_f_frac.merge(region_density,left_index=True,right_index=True)\n",
    "\n",
    "    # calculate the biomass fraction per region by multiplying the forest/nonforest area fraction by the biomass density and normalizing to the sum of both\n",
    "    region_biomass_frac = region_merge.prod(axis=1)/region_merge.prod(axis=1).groupby(['region','time']).sum()\n",
    "    \n",
    "    # rename the index to the region names\n",
    "    ind_map = {x.loc['id']:x.loc['name'] for _,x in metadata.iterrows()}\n",
    "    region_biomass_frac = region_biomass_frac.rename(index=ind_map)\n",
    "\n",
    "    # multiply biomass estimate by biomass fraction to get biomass per region\n",
    "    biomass = biomass.stack()\n",
    "    biomass.name='biomass'\n",
    "    biomass.index.names = ['region','time']\n",
    "    split_biomass = pd.DataFrame(region_biomass_frac,columns=['biomass_frac']).merge(biomass,left_index=True,right_index=True).prod(axis=1).swaplevel(1,2).unstack()\n",
    "    \n",
    "    return split_biomass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Land cover data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESA CCI landcover data\n",
    "files = glob('../results/00_preprocessing/ESA_CCI_landcover_processed_*.nc')\n",
    "CCI_data = xr.open_mfdataset(files)['ESA_CCI_landcover_processed']\n",
    "CCI_data.rio.write_crs('EPSG:4326',inplace=True)\n",
    "\n",
    "# convert the time to year\n",
    "CCI_data['time'] = CCI_data['time'].dt.year\n",
    "\n",
    "# only take data with values larger than 0\n",
    "CCI_data = CCI_data.where(CCI_data>0)\n",
    "\n",
    "# calculate the fraction of each landcover type in each pixel\n",
    "CCI_data = (CCI_data/calc_pixel_area(CCI_data)).rio.set_nodata(np.nan)\n",
    "\n",
    "# for the biomass density estimate, bin the areas into forest and non-forest\n",
    "f_nonf_map = CCI_data.copy()\n",
    "\n",
    "# set the non-forest area as the sum of cropland and shrubland\n",
    "f_nonf_map[0,:,:,:] = CCI_data.sel(landcover=['cropland','shrubland']).sum(dim='landcover')\n",
    "\n",
    "# remove the shrubland dimension\n",
    "f_nonf_map = f_nonf_map[:2,:,:]\n",
    "\n",
    "# rename the landcover types\n",
    "f_nonf_map['landcover'] = ['nonforest','forest']\n",
    "\n",
    "f_nonf_map = f_nonf_map.sel(landcover=['forest','nonforest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Song et al. data\n",
    "song_data = xr.open_dataarray('../results/00_preprocessing/song_et_al_landcover_processed.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wwf ecoregions\n",
    "wwf_ecoregions = gpd.read_file('../results/00_preprocessing/agg_wwf_ecoregions.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load region data from Xu et al.\n",
    "xu_regions = rio.open_rasterio('../data/regions_data/xu_et_al_2021/global_ecoregions.tif').sel(band=1)\n",
    "\n",
    "# replace regions 0,15, and 16 with NaN and convert values to start from 0\n",
    "xu_regions = xr.where(xu_regions.isin([0,15,16]),np.nan,xu_regions-101)\n",
    "xu_regions.rio.write_crs('EPSG:4326',inplace=True);\n",
    "\n",
    "# for the regional analysis, we need to encode the one-hot encode the regions in a different dimension\n",
    "xu_regions_types = np.unique(xu_regions.fillna(0))\n",
    "xu_regions_onehot = xr.concat([(xu_regions==x).astype(float) for x in xu_regions_types],dim='xu_region').drop_vars('band')\n",
    "xu_regions_onehot['xu_region'] = xu_regions_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GFED regions\n",
    "GFED_regions = rio.open_rasterio('../data/regions_data/GFED/GFED5_Beta_monthly_2002.nc',variable='basisregions').sel(band=1)['basisregions']\n",
    "GFED_regions.rio.write_crs(4326,inplace=True);\n",
    "\n",
    "# replace 0 values with NaN\n",
    "GFED_regions = GFED_regions.where(GFED_regions!=0)\n",
    "\n",
    "# set the nodata value to NaN\n",
    "GFED_regions.rio.write_nodata(np.nan,inplace=True);\n",
    "\n",
    "# reproject to the xu_regions resolution because we will use xu_regions in the forest/nonforest split\n",
    "GFED_regions = GFED_regions.rio.reproject_match(xu_regions).drop_vars('band')\n",
    "\n",
    "# load the names of each region\n",
    "GFED_region_names = pd.read_excel('../data/biomass/besnard_et_al_2021/data.xlsx',sheet_name='region_names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Biomass data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Liu et al. (2015)](https://www.nature.com/articles/nclimate2581)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "liu_data = rio.open_rasterio('../data/biomass/liu_et_al_2015/Global_annual_mean_ABC_lc2001_1993_2012_20150331.nc',masked=True)['Aboveground Biomass Carbon']\n",
    "\n",
    "# set the coordinatess to the same as the other datasets\n",
    "liu_data = xr.DataArray(data=liu_data.values.swapaxes(2,1)[:,:,::-1],\n",
    "                    coords=[liu_data['time'].values,np.linspace(89.875,-89.875,720),np.linspace(-179.875,179.875,1440)],\n",
    "                    dims=['time','y','x'])\n",
    "liu_data = liu_data.rio.write_crs(4326)\n",
    "liu_data['time'] = liu_data['time'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Xu et al. (2021)](https://www.science.org/doi/10.1126/sciadv.abe9829)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "xu_data = rio.open_rasterio('../data/biomass/xu_et_al_2021/test10a_cd_ab_pred_corr_2000_2019_v2.tif',masked=True,chunks='auto')\n",
    "\n",
    "# set the year dimension to by integer years\n",
    "xu_data['time'] = xu_data['time'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Chen et al. (2023)](https://essd.copernicus.org/articles/15/897/2023/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "files = [sorted(glob(f'../data/biomass/chen_et_al_2023/DATA/{i}/*.tif')) for i in ['AGBC','BGBC']]\n",
    "chen_agb = xr.open_mfdataset(files[0],concat_dim='time',combine='nested').squeeze()['band_data']\n",
    "chen_bgb = xr.open_mfdataset(files[1],concat_dim='time',combine='nested').squeeze()['band_data']\n",
    "\n",
    "# combine above and below ground biomass\n",
    "chen_data = chen_agb.copy()\n",
    "chen_data[:] = chen_agb.values + chen_bgb.values\n",
    "\n",
    "# set the time dimension to integer years\n",
    "chen_data['time'] = [int(i.split('.')[2][-4:]) for i in files[0]]\n",
    "\n",
    "# set the nodata variable\n",
    "chen_data.rio.set_nodata(np.nan,inplace=True)\n",
    "\n",
    "# down sample to 0.1 degree resolution\n",
    "chen_data = down_sample(chen_data,x_factor=12,y_factor=12,stat='mean')\n",
    "chen_data = chen_data.where(chen_data>0)\n",
    "\n",
    "# drop year 2021\n",
    "chen_data = chen_data.sel(time=chen_data['time']!=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-VOD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LVOD_data = xr.open_dataset('../data/biomass/LVOD/AGC_vod_annual_NOAA_Trend_corrected_lat_lon_merged.nc')['AGC_ASC_DESC']\n",
    "LVOD_data.rio.write_crs(4326,inplace=True);\n",
    "\n",
    "LVOD_data_ASC_DESC = LVOD_data[0,:,:,:]\n",
    "LVOD_data_ASC_DESC_max = LVOD_data[1,:,:,:]\n",
    "LVOD_data_ASC_DESC_min = LVOD_data[2,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Besnard et al. (2021)](https://onlinelibrary.wiley.com/doi/full/10.1111/gcb.15877)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "besnard_data = pd.read_excel('../data/biomass/besnard_et_al_2021/data.xlsx',sheet_name='biomass')\n",
    "besnard_data = besnard_data.set_index(['Region','Year'])['NABP'].unstack().drop(index='Global')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. For gridded data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the specific arguments for each biomass source\n",
    "biomass_sources = [liu_data,xu_data,chen_data, LVOD_data_ASC_DESC,LVOD_data_ASC_DESC_max,LVOD_data_ASC_DESC_min]\n",
    "biomass_names = ['liu_biomass','xu_biomass','chen_biomass','LVOD','LVODmax','LVODmin']\n",
    "area_frac_maps = {'xu':CCI_data,'song':f_nonf_map,'CCI':CCI_data}\n",
    "density_args = {'xu':None,'song':song_data,'CCI':CCI_data}\n",
    "regions = {'xu':xu_regions,'song':wwf_ecoregions,'CCI':wwf_ecoregions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5a65fe6c294d3796686378f3db3b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liu_biomass xu\n",
      "liu_biomass song\n",
      "liu_biomass CCI\n",
      "xu_biomass xu\n",
      "xu_biomass song\n",
      "xu_biomass CCI\n",
      "chen_biomass xu\n",
      "chen_biomass song\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/dask/core.py:127: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/scipy/optimize/_nnls.py:105: RuntimeWarning: overflow encountered in matmul\n",
      "  AtA = A.T @ A\n",
      "/home/ymbaron/data/projects/land_sink_partitioning/.venv/lib/python3.11/site-packages/scipy/optimize/_nnls.py:155: RuntimeWarning: invalid value encountered in matmul\n",
      "  w[:] = Atb - AtA @ x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chen_biomass CCI\n",
      "LVOD xu\n",
      "LVOD song\n",
      "LVOD CCI\n",
      "LVODmax xu\n",
      "LVODmax song\n",
      "LVODmax CCI\n",
      "LVODmin xu\n",
      "LVODmin song\n",
      "LVODmin CCI\n"
     ]
    }
   ],
   "source": [
    "overwrite = True\n",
    "for i, method in itertools.product(range(len(biomass_sources)),['xu','song','CCI']):\n",
    "    print(biomass_names[i],method)\n",
    "    if os.path.exists(f'../results/01_split_forest_nonforest/{biomass_names[i]}_{method}.nc'):\n",
    "        if overwrite == False:\n",
    "            continue\n",
    "    res = split_biomass(biomass     = biomass_sources[i],\n",
    "                        area_frac   = area_frac_maps[method],\n",
    "                        density_args= density_args[method],\n",
    "                        regions     = regions[method],\n",
    "                        method      = method,\n",
    "                        )\n",
    "    res.to_netcdf(f'../results/01_split_forest_nonforest/{biomass_names[i]}_{method}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. For regional data sources\n",
    "\n",
    "The only data source which has regional resolution is Besnard et al. (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = split_biomass_regional(biomass_regions = GFED_regions,\n",
    "                                biomass         = besnard_data*1e15,\n",
    "                                metadata        = GFED_region_names,\n",
    "                                density_regions = xu_regions_onehot,\n",
    "                                forest_map      = f_nonf_map\n",
    "                                )\n",
    "\n",
    "result.to_csv(f'../results/01_split_forest_nonforest/besnard_biomass_regional.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
