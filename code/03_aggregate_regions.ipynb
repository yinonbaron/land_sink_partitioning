{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rioxarray as rio\n",
    "import geopandas as gpd\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import Union\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data into RECCAP regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "We aggregate the data into the RECCAP regions. For gridded data sources we use a GeoDataFrame that defines the RECCAP regions. The GeoDataFrame is created in the preprocessing notebook `00c_define_regions.ipynb`\n",
    "\n",
    "For regional data sources, in case the regions match with RECCAP regions (for example country level data), we just sum all regions that is found in each RECCAP region. The mapping between country level data and RECCAP regions is also defined in the preprocessing notebook `00c_define_regions.ipynb`\n",
    "\n",
    "In case the regions do not match with RECCAP regions, we use our gridded data source to estimate the contribution of each region to the respective RECCAP regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load country data\n",
    "countries_data = gpd.read_file('../data/country_data/country_data_w_RECCAP_Pan_FAO.shp')\n",
    "countries_data['id'] = countries_data.index\n",
    "# create reccap_regions\n",
    "reccap_regions = countries_data.dissolve(by='RECCAP reg')\n",
    "\n",
    "# create pan_regions\n",
    "pan_regions = countries_data.dissolve(by='Pan region')\n",
    "pan_regions['name'] = pan_regions.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GFED regions\n",
    "GFED_regions = rio.open_rasterio('../data/regions_data/GFED/GFED5_Beta_monthly_2002.nc',variable='basisregions').sel(band=1)['basisregions']\n",
    "GFED_regions.rio.write_crs(4326,inplace=True);\n",
    "\n",
    "# replace 0 values with NaN\n",
    "GFED_regions = GFED_regions.where(GFED_regions!=0)\n",
    "\n",
    "# set the nodata value to NaN\n",
    "GFED_regions.rio.write_nodata(np.nan,inplace=True);\n",
    "\n",
    "# load the names of each region\n",
    "GFED_region_names = pd.read_excel('../data/biomass/besnard_et_al_2021/data.xlsx',sheet_name='region_names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define functions for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. For gridded data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gridded(regions:gpd.GeoDataFrame, biomass:xr.DataArray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sum gridded biomass data to regions.\n",
    "\n",
    "    Parameters:\n",
    "    regions (gpd.GeoDataFrame): The region definition.\n",
    "    biomass (xr.DataArray): The biomass data to analyze.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The analyzed biomass data.\n",
    "    \"\"\"\n",
    "\n",
    "    ## 1. check if dataset covers mainly one region\n",
    "\n",
    "    # calculate the total area covred by each region in the biomass dataset\n",
    "    area_fracs = raster_vector_zonal_stats(regions.reset_index(),calc_area(biomass).where(biomass.sum(dim=['landcover','time'])>0),'sum')\n",
    "    \n",
    "    # if one region is dominating the area, aggregate to that region\n",
    "    if (area_fracs/area_fracs.sum()).max() > 0.95:\n",
    "\n",
    "        # calculate the total biomass in the data set\n",
    "        res =(biomass*calc_area(biomass)*100).sum(dim=['x','y']).to_dataframe(name='biomass')['biomass'].unstack()\n",
    "\n",
    "        # find the region with the largest area fraction from the regions GeoDataFrame\n",
    "        r = regions.index[area_fracs.index[area_fracs.argmax()].astype(int)]\n",
    "\n",
    "        # merge the index of the region with the index of the sum of the biomass (which is the landcover type)\n",
    "        res.index = pd.MultiIndex.from_product([[r],res.index])\n",
    "\n",
    "        return res\n",
    "    \n",
    "    # else, aggregate to all regions\n",
    "    else:\n",
    "\n",
    "        # calculate the sum of the biomass in each region\n",
    "        res = raster_vector_zonal_stats(regions.reset_index(),biomass*calc_area(biomass)*100,'sum',interp=True).unstack()\n",
    "\n",
    "        # rename the indices to be the region names and the landcover types\n",
    "        res.index =pd.MultiIndex.from_product([regions.index,biomass['landcover'].values])\n",
    "\n",
    "        # set the columns as the time values\n",
    "        res.columns = biomass['time'].values\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. For regional data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conversion(crosstab:gpd.GeoDataFrame,biomass:xr.DataArray) -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculate the conversion factors between regions and RECCAP regions for a given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    crosstab (gpd.GeoDataFrame): The intersection of the regions and the RECCAP regions.\n",
    "    biomass (xr.DataArray): The biomass data to analyze.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The conversion factors per region, RECCAP region and landcover type.\n",
    "    '''\n",
    "    \n",
    "    # calculate the total biomass in each region\n",
    "    region_biomass = raster_vector_zonal_stats(crosstab,biomass*calc_area(biomass),'sum').unstack()\n",
    "    \n",
    "    # merge with the original regions GeoDataFrame to get the names of the regions and then set the index of the DataFrame to be the region names, the RECCAP region names, and the landcover types\n",
    "    region_biomass = region_biomass.reset_index().set_index('index').merge(crosstab[['name','RECCAP reg']],left_index=True,right_index=True).set_index(['name','RECCAP reg','landcover'])\n",
    "    \n",
    "    # calculate the fraction of biomass in each region that is in each RECCAP region per landcover type\n",
    "    conversion_df = region_biomass/region_biomass.groupby(['name','landcover']).sum()\n",
    "    \n",
    "    # return constant coversion factors in time\n",
    "#     result = pd.DataFrame(0,index=conversion_df.index,columns=conversion_df.columns)\n",
    "#     result = result.add(conversion_df.mean(axis=1),axis=0)\n",
    "    result = conversion_df.mean(axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def analyze_dataset_regional(biomass:pd.DataFrame,region:Union[xr.DataArray,gpd.GeoDataFrame],metadata) -> pd.DataFrame:\n",
    "\n",
    "        # if the region is a raster, polygonize it into a GeoDataFrame\n",
    "        if type(region) == xr.DataArray:\n",
    "                region = polygonize(region)\n",
    "\n",
    "        # calculate the intersection of the regions and the RECCAP regions\n",
    "        crosstab = reccap_regions.reset_index()[['RECCAP reg','geometry']].overlay(region.reset_index()).merge(metadata,left_on='id',right_on='id')\n",
    "        \n",
    "        # take only regions with area larger than 50 Mha\n",
    "        crosstab['area'] = crosstab.to_crs(epsg=6933).area\n",
    "        crosstab = crosstab[crosstab['area']>5e11]\n",
    "        \n",
    "        # take all of the global gridded biomass estimates (Liu et al., Xu et al., LVOD data)\n",
    "        biomass_files =  [x for x in glob('../results/02_convert_AGB_TB/*.nc') if ('chen' not in x)]\n",
    "        \n",
    "        # calculate the conversion factors between regions and RECCAP regions for each dataset\n",
    "        conversions = pd.concat([calculate_conversion(crosstab,xr.open_dataarray(f)) for f in biomass_files])\n",
    "\n",
    "        # calculate the mean conversion across all datasets\n",
    "        mean_conversion = conversions.groupby(['name','RECCAP reg','landcover']).mean()\n",
    "\n",
    "        # rename indices\n",
    "        mean_conversion.index.names = ['region','RECCAP reg','landcover']\n",
    "        biomass.index.names = ['region','landcover']\n",
    "\n",
    "        # apply the conversion to get the biomass in the RECCAP regions\n",
    "        result = biomass.mul(mean_conversion,axis=0).dropna().groupby(['RECCAP reg','landcover']).sum()\n",
    "\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. For DGVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_TRENDY(fn:str) -> xr.DataArray:\n",
    "    '''\n",
    "    Preprocess TRENDY data for analysis.\n",
    "\n",
    "    Parameters:\n",
    "    fn (str): The file path to the TRENDY data.\n",
    "\n",
    "    Returns:\n",
    "    xr.DataArray: The preprocessed TRENDY data.\n",
    "    '''\n",
    "\n",
    "    variable = fn.split('/')[-1].split('_')[2]\n",
    "\n",
    "    # load data\n",
    "    da = xr.open_dataset(fn)\n",
    "    # get model names\n",
    "    models = da.attrs['models'].split(' ')\n",
    "\n",
    "    # get NBP data from the dataset\n",
    "    da = da[variable]\n",
    "\n",
    "    # assign model names and time in years\n",
    "    da['model'] = models\n",
    "    da['time'] = da['time'].dt.year\n",
    "\n",
    "    # change the name of the dimensions to x and y\n",
    "    da = da.rename({'lon':'x','lat':'y'})\n",
    "\n",
    "    if variable == 'nbp':\n",
    "        # convert units from kgC/m2/s to gC/m2/yr\n",
    "        da = da * 60*60*24*365*1e3\n",
    "    else:\n",
    "        da = da * 1e3\n",
    "\n",
    "    # set CRS\n",
    "    da.rio.write_crs(4326,inplace=True);\n",
    "\n",
    "    # transpose time and model dimensions\n",
    "    da = da.transpose('model','time','y','x')\n",
    "\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gridded_DGVM(region:gpd.GeoDataFrame, biomass:xr.DataArray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sum gridded biomass data to regions.\n",
    "\n",
    "    Parameters:\n",
    "    regions (gpd.GeoDataFrame): The region definition.\n",
    "    biomass (xr.DataArray): The biomass data to analyze.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The analyzed biomass data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the sum of the biomass in each region\n",
    "    res = raster_vector_zonal_stats(region.reset_index(),biomass*calc_area(biomass),'sum',interp=True).unstack()\n",
    "    \n",
    "    # set indices, columns and indices names\n",
    "    res.index =pd.MultiIndex.from_product([region.index,biomass['model'].values])\n",
    "    res.columns = biomass['time'].values\n",
    "    res.index.names = ['region','model']\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. For gridded data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [25:03<00:00, 16.70s/it]\n"
     ]
    }
   ],
   "source": [
    "overwrite = True\n",
    "files =  glob('../results/02_convert_AGB_TB/*.nc')\n",
    "for file in tqdm(files):\n",
    "    if os.path.exists(f'../results/03_aggregate_regions/{file.split(\"/\")[-1][:-3]}_regions.csv'):\n",
    "        if overwrite == False:\n",
    "            print(f'../results/03_aggregate_regions/{file.split(\"/\")[-1][:-3]}_regions.csv already exists')\n",
    "            continue \n",
    "    res = analyze_gridded(reccap_regions,xr.open_dataarray(file,decode_times=False).fillna(0))\n",
    "    res.to_csv(f'../results/03_aggregate_regions/{file.split(\"/\")[-1][:-3]}_regions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. For regional data sources\n",
    "\n",
    "We have three regional data sources to analyze:\n",
    "\n",
    "1. Besnard et al. (2021)\n",
    "\n",
    "2. Pan et al. (2011)\n",
    "\n",
    "3. FRA data from Tubiello et al. (2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Besnard et al. (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/ymbaron/data/projects/land_sink_revised/.venv/lib/python3.11/site-packages/xarray/core/duck_array_ops.py:216: RuntimeWarning: invalid value encountered in cast\n",
      "  return data.astype(dtype, **kwargs)\n",
      " 50%|█████     | 1/2 [07:51<07:51, 471.31s/it]/home/ymbaron/data/projects/land_sink_revised/.venv/lib/python3.11/site-packages/xarray/core/duck_array_ops.py:216: RuntimeWarning: invalid value encountered in cast\n",
      "  return data.astype(dtype, **kwargs)\n",
      "100%|██████████| 2/2 [16:49<00:00, 504.67s/it]\n"
     ]
    }
   ],
   "source": [
    "files = glob('../results/02_convert_AGB_TB/besnard*.csv')\n",
    "for file in tqdm(files):\n",
    "    df = pd.read_csv(file,index_col=[0,1])\n",
    "    df.columns = df.columns.astype(int)\n",
    "    res = analyze_dataset_regional(df,GFED_regions,GFED_region_names)\n",
    "    res.to_csv(f'../results/03_aggregate_regions/{file.split(\"/\")[-1].split(\".\")[0]}_regions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Pan et al. (2011)\n",
    "\n",
    "For Pan et al., all regions are consistent with the RECCAP regions, except from the \"Americas\". For all other regions expect \"Americas\", we can directly aggregate the data.\n",
    "\n",
    "The \"Americas\" region contains Central America. Central America is considered in the \"North America\" RECCAP region. We calculate a conversion factor between the two regions and apply it to the \"Americas\" region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from Table S3 in Pan et al.\n",
    "pan_data = pd.read_excel('../data/biomass/pan_et_al_2011/data.xlsx')\n",
    "\n",
    "# take the \"Total living biomass\" data and exclude the \"Tropical Intact\" and \"Tropical Regrowth\" biomes (as they are included in the \"All Tropics\" rows)\n",
    "pan_data_df = pan_data[(pan_data.Type=='Total living biomass') & (~pan_data.Biome.isin(['Tropical Intact','Tropical Regrowth']))].pivot_table(index='Region',columns='Year',values='Carbon',aggfunc='sum')\n",
    "\n",
    "# interpolate the data to fill in missing years\n",
    "pan_data_df = (pd.DataFrame(0,index=pan_data_df.index,columns=np.arange(1990,2008),dtype=float)+pan_data_df.astype(float)).interpolate(axis=1)*1e15\n",
    "\n",
    "# set landcover to forest, and set the index to region and landcover\n",
    "pan_data_df['landcover'] = 'forest'\n",
    "pan_data_df = pan_data_df.reset_index().set_index(['Region','landcover'])\n",
    "pan_data_df.index.names = ['name','landcover']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymbaron/data/projects/land_sink_revised/.venv/lib/python3.11/site-packages/geopandas/geodataframe.py:2475: UserWarning: `keep_geom_type=True` in overlay resulted in 1 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  return geopandas.overlay(\n"
     ]
    }
   ],
   "source": [
    "# load data from Table S3 in Pan et al.\n",
    "pan_data = pd.read_excel('../data/biomass/pan_et_al_2011/data.xlsx')\n",
    "\n",
    "# take the \"Total living biomass\" data and exclude the \"Tropical Intact\" and \"Tropical Regrowth\" biomes (as they are included in the \"All Tropics\" rows)\n",
    "pan_data_df = pan_data[(pan_data.Type=='Total living biomass') & (~pan_data.Biome.isin(['Tropical Intact','Tropical Regrowth']))].pivot_table(index='Region',columns='Year',values='Carbon',aggfunc='sum')\n",
    "\n",
    "# interpolate the data to fill in missing years\n",
    "pan_data_df = (pd.DataFrame(0,index=pan_data_df.index,columns=np.arange(1990,2008),dtype=float)+pan_data_df.astype(float)).interpolate(axis=1)*1e15\n",
    "\n",
    "# set landcover to forest, and set the index to region and landcover\n",
    "pan_data_df['landcover'] = 'forest'\n",
    "pan_data_df = pan_data_df.reset_index().set_index(['Region','landcover'])\n",
    "pan_data_df.index.names = ['name','landcover']\n",
    "\n",
    "# for all regions except \"Americas\", the conversion factor is 1\n",
    "conversion = pan_data_df/pan_data_df\n",
    "conversion = conversion.merge(pan_data[['Region','RECCAP region']].drop_duplicates(),left_on='name',right_on='Region').dropna().set_index(['Region','RECCAP region'])\n",
    "conversion.index.names = ['name','RECCAP reg']\n",
    "\n",
    "# calculate the biomass in RECCAP regions for the fully matching regions\n",
    "pan_RECCAP = pan_data_df.mul(conversion,axis=0).groupby(['RECCAP reg','landcover']).sum()\n",
    "\n",
    "# calculate the conversion factors for the \"Americas\" region\n",
    "pan_conv = analyze_dataset_regional(pan_data_df,pan_regions.drop(columns=['name','RECCAP reg']),pan_regions.reset_index()[['name','id']])\n",
    "\n",
    "# concatenate the fully matching and non matching regions\n",
    "pan_RECCAP = pd.concat([pan_RECCAP,pan_conv])\n",
    "\n",
    "# sum over RECCAP regions and landcover types\n",
    "pan_RECCAP = pan_RECCAP.groupby(['RECCAP reg','landcover']).sum()\n",
    "\n",
    "# save data\n",
    "pan_RECCAP.to_csv('../results/03_aggregate_regions/pan_regions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. FRA data from Tubiello et al. (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "FRA_data = pd.read_csv('../data/biomass/tubiello_et_al_2021/GF_GHG_ForestLand_Total_2020_ZENODO.csv', encoding='latin-1')\n",
    "\n",
    "# Fix the name for Sudan to match the FAO names\n",
    "FRA_data['AreaName'] = FRA_data['AreaName'].str.replace(' (former)', '', regex=False)\n",
    "\n",
    "# Take the stocks data and convert to a DataFrame with the region names as the index and years as columns\n",
    "FRA_data = FRA_data[FRA_data['ElementName'] == 'Carbon Stock (million tonnes)']\n",
    "FRA_data = FRA_data[['AreaName','Year','Value']].groupby(['AreaName','Year']).mean()['Value'].unstack()\n",
    "\n",
    "# convert units from MtC to gC\n",
    "FRA_data = FRA_data*1e12\n",
    "\n",
    "# merge data with country data\n",
    "FRA_data_merge = FRA_data.merge(countries_data[['FAO_name','RECCAP reg']],left_index=True,right_on='FAO_name',how='left')\n",
    "\n",
    "# calculate total biomass for each RECCAP region\n",
    "FRA_RECCAP_sum = FRA_data_merge.groupby('RECCAP reg').sum()\n",
    "\n",
    "# set index to be the RECCAP region and landcover type, which is forest\n",
    "FRA_RECCAP_sum['landcover'] = 'forest'\n",
    "FRA_RECCAP_sum = FRA_RECCAP_sum.reset_index().set_index(['RECCAP reg','landcover'])\n",
    "\n",
    "# drop the FAO_name column\n",
    "FRA_RECCAP_sum.drop(columns='FAO_name',inplace=True)\n",
    "\n",
    "# save data\n",
    "FRA_RECCAP_sum.to_csv('../results/03_aggregate_regions/FRA_regions.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Analyze DGVM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 cVeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess data\n",
    "cVeg_grid = preprocess_TRENDY('../data/DGVMs/trendyv10_S3_cVeg_1901-2020_annual_gridded.nc')\n",
    "\n",
    "# run the analysis\n",
    "trendy_cVeg_regional = analyze_gridded_DGVM(reccap_regions,cVeg_grid)\n",
    "\n",
    "# find missing data and remove it\n",
    "\n",
    "# calculate the sum for each model\n",
    "model_sum = trendy_cVeg_regional.groupby('model').sum().sum(axis=1)\n",
    "\n",
    "# valid models have a sum different than 0\n",
    "valid_models = model_sum[model_sum!=0].index\n",
    "\n",
    "# remove invalid models\n",
    "trendy_cVeg_regional = trendy_cVeg_regional[trendy_cVeg_regional.index.get_level_values('model').isin(valid_models)]\n",
    "\n",
    "# save the results\n",
    "trendy_cVeg_regional.to_csv('../results/03_aggregate_regions/regional_DGVM_cVeg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 NBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess data\n",
    "nbp_grid = preprocess_TRENDY('../data/DGVMs/trendyv10_S3_nbp_1901-2020_annual_gridded.nc')\n",
    "\n",
    "# run the analysis\n",
    "trendy_nbp_regional = analyze_gridded_DGVM(reccap_regions,nbp_grid)\n",
    "\n",
    "# save the results\n",
    "trendy_nbp_regional.to_csv('../results/03_aggregate_regions/regional_DGVM_nbp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
